{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c5e6c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1641e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Cargar el archivo CSV original ---\n",
    "input_file = \"Datasets/diabetes_012_health_indicators_BRFSS2015.csv\"\n",
    "data = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d3637b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Preprocesar los datos ---\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Guardar los rangos originales para desescalar después\n",
    "column_ranges = {\n",
    "    col: (data[col].min(), data[col].max()) for col in data.columns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "56e22f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dividir los datos en entrenamiento y validación ---\n",
    "train_data, val_data = train_test_split(data_scaled, test_size=0.2, random_state=42)\n",
    "# Convertir los datos a tensores de PyTorch\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "val_tensor = torch.tensor(val_data, dtype=torch.float32)\n",
    "\n",
    "# Índices de las columnas importantes\n",
    "bmi_index = data.columns.get_loc(\"BMI\")\n",
    "menthlth_index = data.columns.get_loc(\"MentHlth\")\n",
    "physhlth_index = data.columns.get_loc(\"PhysHlth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0528d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Definir el modelo GAN ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):  # Corrección: falta la coma\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a027123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones\n",
    "latent_dim = 100\n",
    "data_dim = train_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ba3bb82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar el generador y el discriminador\n",
    "generator = Generator(latent_dim, data_dim)\n",
    "discriminator = Discriminator(data_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "51d9eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Configurar el entrenamiento ---\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0001)\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "epochs = 10000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0747b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000 | D Loss: 1.3652408123016357 | G Loss: 0.7153164148330688\n",
      "Epoch 500/10000 | D Loss: 1.0837342739105225 | G Loss: 0.914198637008667\n",
      "Epoch 1000/10000 | D Loss: 1.25753915309906 | G Loss: 0.6913037300109863\n",
      "Epoch 1500/10000 | D Loss: 1.362099289894104 | G Loss: 0.8201712369918823\n",
      "Epoch 2000/10000 | D Loss: 1.3366727828979492 | G Loss: 0.826366126537323\n",
      "Epoch 2500/10000 | D Loss: 1.25968337059021 | G Loss: 0.9821163415908813\n",
      "Epoch 3000/10000 | D Loss: 1.487748146057129 | G Loss: 0.6618410348892212\n",
      "Epoch 3500/10000 | D Loss: 1.5428130626678467 | G Loss: 0.8575720191001892\n",
      "Epoch 4000/10000 | D Loss: 0.19278669357299805 | G Loss: 2.979649543762207\n",
      "Epoch 4500/10000 | D Loss: 0.04733400046825409 | G Loss: 4.327676296234131\n",
      "Epoch 5000/10000 | D Loss: 0.04631864279508591 | G Loss: 4.462451934814453\n",
      "Epoch 5500/10000 | D Loss: 0.08297702670097351 | G Loss: 3.310225486755371\n",
      "Epoch 6000/10000 | D Loss: 0.2304740995168686 | G Loss: 3.860424280166626\n",
      "Epoch 6500/10000 | D Loss: 0.013888014480471611 | G Loss: 5.205780029296875\n",
      "Epoch 7000/10000 | D Loss: 0.0014936368679627776 | G Loss: 6.670408248901367\n",
      "Epoch 7500/10000 | D Loss: 0.1081060990691185 | G Loss: 6.182601451873779\n",
      "Epoch 8000/10000 | D Loss: 0.014200497418642044 | G Loss: 5.433589935302734\n",
      "Epoch 8500/10000 | D Loss: 0.0028365582693368196 | G Loss: 6.677872180938721\n",
      "Epoch 9000/10000 | D Loss: 0.002992141293361783 | G Loss: 7.305262088775635\n",
      "Epoch 9500/10000 | D Loss: 0.0007550636655651033 | G Loss: 7.757660388946533\n"
     ]
    }
   ],
   "source": [
    "g_losses_train = []\n",
    "d_losses_train = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Entrenar el discriminador\n",
    "    real_data = train_tensor[torch.randint(0, train_tensor.size(0), (batch_size,))]\n",
    "    real_labels = torch.ones((batch_size, 1))\n",
    "    fake_latent = torch.randn((batch_size, latent_dim))\n",
    "    fake_data = generator(fake_latent)\n",
    "    fake_labels = torch.zeros((batch_size, 1))\n",
    "\n",
    "    optimizer_d.zero_grad()\n",
    "    real_loss = criterion(discriminator(real_data), real_labels)\n",
    "    fake_loss = criterion(discriminator(fake_data.detach()), fake_labels)\n",
    "    d_loss = real_loss + fake_loss\n",
    "    d_loss.backward()\n",
    "    optimizer_d.step()\n",
    "\n",
    "    # Entrenar el generador\n",
    "    optimizer_g.zero_grad()\n",
    "    fake_labels = torch.ones((batch_size, 1))\n",
    "    g_loss = criterion(discriminator(fake_data), fake_labels)\n",
    "    g_loss.backward()\n",
    "    optimizer_g.step()\n",
    "\n",
    "    # Guardar las pérdidas de entrenamiento\n",
    "    g_losses_train.append(g_loss.item())\n",
    "    d_losses_train.append(d_loss.item())\n",
    "\n",
    "    # Mostrar progreso\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs} | D Loss: {d_loss.item()} | G Loss: {g_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "da5b4970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevos datos generados y guardados en: Datasets/generated_data2.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Generar nuevos datos ---\n",
    "num_samples = 1000\n",
    "latent_samples = torch.randn((num_samples, latent_dim))\n",
    "generated_data = generator(latent_samples).detach().numpy()\n",
    "\n",
    "# Desescalar cada columna según su rango original\n",
    "for i, col in enumerate(data.columns):\n",
    "    min_val, max_val = column_ranges[col]\n",
    "    generated_data[:, i] = generated_data[:, i] * (max_val - min_val) + min_val\n",
    "\n",
    "# Aplicar la función techo para redondear los datos a enteros\n",
    "generated_data = np.ceil(generated_data)\n",
    "\n",
    "# Limitar los valores generados al rango válido para Diabetes_012\n",
    "generated_data[:, 0] = np.clip(generated_data[:, 0], 0, 2)  # Columna Diabetes_012\n",
    "\n",
    "# Guardar los datos generados en un nuevo archivo CSV\n",
    "output_file = \"Datasets/generated_data2.csv\"\n",
    "pd.DataFrame(generated_data, columns=data.columns).to_csv(output_file, index=False)\n",
    "print(f\"Nuevos datos generados y guardados en: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
