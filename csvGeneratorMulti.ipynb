{"cells":[{"cell_type":"code","execution_count":3,"id":"t7O_2_dM_Cn2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2247,"status":"ok","timestamp":1751576648163,"user":{"displayName":"Sebastián Ignacio García Péndola","userId":"08175071328414149400"},"user_tz":240},"id":"t7O_2_dM_Cn2","outputId":"add78291-aec0-47d6-d709-ba8a3ffaab57"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"id":"c56e94ad","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c56e94ad","outputId":"6d4fb91f-fba3-427f-c006-65e160dc8ebf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Usando dispositivo: cpu\n","Unique values in 'Diabetes_012' after stripping and mapping:\n","['F' 'M']\n","NaN values in 'Diabetes_012' after stripping and mapping:\n","0\n","\n","--- Conteo de valores para CLASS (Original antes de SMOTE): ---\n","CLASS\n","0    103\n","1     53\n","2    844\n","Name: count, dtype: int64\n","\n","Iniciando balanceo con SMOTE...\n","\n","--- Conteo de valores para CLASS (Después de SMOTE): ---\n","CLASS\n","0    844\n","1    844\n","2    844\n","Name: count, dtype: int64\n","Iniciando preprocesamiento...\n","Preprocesado CLASS con One-Hot Encoding. Shape: (2532, 3)\n","Preprocesado Gender con One-Hot Encoding. Shape: (2532, 2)\n","Preprocesado AGE con log1p + MinMaxScaler. Shape: (2532, 1)\n","Preprocesado BMI con log1p + MinMaxScaler. Shape: (2532, 1)\n","Preprocesado Urea con MinMaxScaler. Shape: (2532, 1)\n","Preprocesado Cr con MinMaxScaler. Shape: (2532, 1)\n","Preprocesado HbA1c con MinMaxScaler. Shape: (2532, 1)\n","Preprocesado Chol con MinMaxScaler. Shape: (2532, 1)\n","Preprocesado TG con MinMaxScaler. Shape: (2532, 1)\n","Preprocesado HDL con MinMaxScaler. Shape: (2532, 1)\n","Preprocesado LDL con MinMaxScaler. Shape: (2532, 1)\n","Preprocesado VLDL con MinMaxScaler. Shape: (2532, 1)\n","Forma final de los datos procesados (X_train_processed_np): (2532, 15)\n","\n","--- Arquitectura del Generador (PyTorch) ---\n","Generator(\n","  (model): Sequential(\n","    (0): Linear(in_features=100, out_features=512, bias=True)\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Dropout(p=0.4, inplace=False)\n","    (3): BatchNorm1d(512, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n","    (4): Linear(in_features=512, out_features=1024, bias=True)\n","    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (6): Dropout(p=0.3, inplace=False)\n","    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n","    (8): Linear(in_features=1024, out_features=2048, bias=True)\n","    (9): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (10): Dropout(p=0.2, inplace=False)\n","    (11): BatchNorm1d(2048, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n","    (12): Linear(in_features=2048, out_features=4096, bias=True)\n","    (13): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (14): Dropout(p=0.1, inplace=False)\n","    (15): BatchNorm1d(4096, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n","    (16): Linear(in_features=4096, out_features=15, bias=True)\n","    (17): Sigmoid()\n","  )\n",")\n","\n","--- Arquitectura del Discriminador (PyTorch) ---\n","Discriminator(\n","  (model): Sequential(\n","    (0): Linear(in_features=15, out_features=1024, bias=True)\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Linear(in_features=1024, out_features=512, bias=True)\n","    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (4): Dropout(p=0.3, inplace=False)\n","    (5): Linear(in_features=512, out_features=1, bias=True)\n","    (6): Sigmoid()\n","  )\n",")\n","\n","Iniciando entrenamiento de la GAN con PyTorch...\n","1/500 [D loss: 0.6879] [G loss: 0.7230]\n","2/500 [D loss: 0.6890] [G loss: 0.7175]\n","3/500 [D loss: 0.6945] [G loss: 0.6777]\n","4/500 [D loss: 0.6987] [G loss: 0.6748]\n","5/500 [D loss: 0.6938] [G loss: 0.6903]\n","6/500 [D loss: 0.6921] [G loss: 0.6937]\n","7/500 [D loss: 0.6922] [G loss: 0.6959]\n","8/500 [D loss: 0.6907] [G loss: 0.6952]\n","9/500 [D loss: 0.6922] [G loss: 0.6922]\n","10/500 [D loss: 0.6880] [G loss: 0.7040]\n","11/500 [D loss: 0.6921] [G loss: 0.6967]\n","12/500 [D loss: 0.6917] [G loss: 0.6968]\n","13/500 [D loss: 0.6946] [G loss: 0.6894]\n","14/500 [D loss: 0.6911] [G loss: 0.6875]\n","15/500 [D loss: 0.6944] [G loss: 0.6868]\n","16/500 [D loss: 0.6973] [G loss: 0.6767]\n","17/500 [D loss: 0.6892] [G loss: 0.7168]\n","18/500 [D loss: 0.6834] [G loss: 0.7146]\n","19/500 [D loss: 0.7053] [G loss: 0.6759]\n","20/500 [D loss: 0.6905] [G loss: 0.6994]\n","21/500 [D loss: 0.6869] [G loss: 0.6886]\n","22/500 [D loss: 0.6941] [G loss: 0.6848]\n","23/500 [D loss: 0.6823] [G loss: 0.7320]\n","24/500 [D loss: 0.6903] [G loss: 0.6896]\n","25/500 [D loss: 0.6895] [G loss: 0.7034]\n","26/500 [D loss: 0.6903] [G loss: 0.7060]\n","27/500 [D loss: 0.6935] [G loss: 0.6863]\n","28/500 [D loss: 0.6943] [G loss: 0.6874]\n","29/500 [D loss: 0.6888] [G loss: 0.7080]\n","30/500 [D loss: 0.6971] [G loss: 0.6816]\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.preprocessing import MinMaxScaler\n","# from sklearn.model_selection import train_test_split # No se usa directamente para GAN\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","# from torch.utils.data import DataLoader, TensorDataset # No se usará DataLoader para este ejemplo simple\n","\n","# --- Importar SMOTE ---\n","from imblearn.over_sampling import SMOTE # Importa SMOTE\n","\n","# --- 0. Parámetros de la GAN y del Entrenamiento ---\n","LATENT_DIM = 100\n","EPOCHS = 500 # Puede necesitar muchas más (ej. 10000-50000+) y ajustes\n","BATCH_SIZE = 64\n","SAMPLE_INTERVAL = 1 # Cada cuántas épocas guardar una muestra de datos generados\n","LEARNING_RATE_G = 0.0002\n","LEARNING_RATE_D = 0.00001\n","BETA1 = 0.5 # Parámetro de Adam\n","\n","iteration = 2\n","generator_path = \"/content/gdrive/My Drive/ResultCSV/Multi/generator_gan_pytorch.pth\"\n","discriminator_path = \"/content/gdrive/My Drive/ResultCSV/Multi/discriminator_gan_pytorch.pth\"\n","\n","# Configurar dispositivo (GPU si está disponible)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Usando dispositivo: {device}\")\n","\n","# --- 1. Cargar el archivo CSV original ---\n","input_file = '/content/gdrive/My Drive/Datasets/datasetMulti.csv'\n","data = pd.read_csv(input_file)\n","data = data.drop(columns=['ID', 'No_Pation'])\n","data['CLASS'] = data['CLASS'].astype(str).str.strip()\n","data['Gender'] = data['Gender'].astype(str).str.strip().str.upper()\n","\n","# --- Verify unique values and NaNs after stripping and mapping ---\n","print(\"Unique values in 'Diabetes_012' after stripping and mapping:\")\n","print(data['Gender'].unique())\n","print(\"NaN values in 'Diabetes_012' after stripping and mapping:\")\n","print(data['CLASS'].isnull().sum())\n","\n","# Mapear valores de 'Gender' a numéricos y luego aplicar One-Hot Encoding si es necesario\n","# Para 'Gender', puedes mapear 'F' a 0 y 'M' a 1.\n","data['Gender'] = data['Gender'].map({'F': 0, 'M': 1})\n","\n","# Mapear 'CLASS' (ahora 'CLASS') a valores numéricos si no lo están ya\n","# Asumiendo 'N'=0, 'P'=1, 'Y'=2\n","class_mapping = {'N': 0, 'P': 1, 'Y': 2}\n","data['CLASS'] = data['CLASS'].map(class_mapping)\n","\n","# class_0 = data[data['CLASS'] == 0].sample(n=53, random_state=42)\n","# class_1 = data[data['CLASS'] == 1].sample(n=53, random_state=42)\n","# class_2 = data[data['CLASS'] == 2].sample(n=53, random_state=42)\n","# data_df = pd.concat([class_0, class_1, class_2]).sample(frac=1, random_state=42).reset_index(drop=True)\n","original_columns = data.columns.tolist()\n","\n","# --- Verificar el conteo de clases antes de SMOTE ---\n","print(\"\\n--- Conteo de valores para CLASS (Original antes de SMOTE): ---\")\n","print(data['CLASS'].value_counts().sort_index())\n","\n","# --- 2. Aplicar SMOTE para balancear las clases ---\n","print(\"\\nIniciando balanceo con SMOTE...\")\n","\n","# Separar características (X) y variable objetivo (y)\n","X_smote = data.drop('CLASS', axis=1)\n","y_smote = data['CLASS']\n","\n","# Definir la estrategia de muestreo: queremos que todas las clases tengan 840 muestras\n","sampling_strategy = {0: 844, 1: 844, 2: 844} # Asegura que la clase 2 no se reduzca si ya tiene 840\n","\n","smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_smote, y_smote)\n","\n","# Recombinar los datos balanceados en un DataFrame para el preprocesamiento de la GAN\n","data_df = pd.DataFrame(X_resampled, columns=X_smote.columns)\n","data_df['CLASS'] = y_resampled\n","\n","# --- Verificar el conteo de clases después de SMOTE ---\n","print(\"\\n--- Conteo de valores para CLASS (Después de SMOTE): ---\")\n","print(data_df['CLASS'].value_counts().sort_index())\n","\n","# --- 3. Preprocesamiento ---\n","print(\"Iniciando preprocesamiento...\")\n","processed_data_parts = []\n","scalers_dict = {}\n","column_info_for_generator_output = []\n","\n","special_cols_log_scale = ['AGE', 'BMI']\n","diabetes_col = 'CLASS'\n","gender_col = 'Gender' # Nueva columna categórica\n","\n","# A. CLASS (One-Hot Encoding)\n","#    Para PyTorch, no necesitamos to_categorical de Keras, podemos hacerlo con numpy\n","num_classes_diabetes = data_df[diabetes_col].nunique() # Obtener el número de clases dinámicamente\n","diabetes_one_hot = np.eye(num_classes_diabetes)[data_df[diabetes_col].astype(int)]\n","processed_data_parts.append(diabetes_one_hot)\n","column_info_for_generator_output.append({'name': diabetes_col, 'type': 'one_hot', 'num_classes': num_classes_diabetes})\n","print(f\"Preprocesado {diabetes_col} con One-Hot Encoding. Shape: {diabetes_one_hot.shape}\")\n","\n","# B. Gender (One-Hot Encoding)\n","num_classes_gender = data_df[gender_col].nunique() # Debería ser 2 (M/F)\n","gender_one_hot = np.eye(num_classes_gender)[data_df[gender_col].astype(int)]\n","processed_data_parts.append(gender_one_hot)\n","column_info_for_generator_output.append({'name': gender_col, 'type': 'one_hot', 'num_classes': num_classes_gender})\n","print(f\"Preprocesado {gender_col} con One-Hot Encoding. Shape: {gender_one_hot.shape}\")\n","\n","# C. Columnas con log1p + MinMaxScaler\n","for col_name in special_cols_log_scale:\n","    original_values = data_df[col_name].values.reshape(-1, 1)\n","    log_transformed_values = np.log1p(original_values)\n","    scaler = MinMaxScaler(feature_range=(0, 1))\n","    scaled_values = scaler.fit_transform(log_transformed_values)\n","    processed_data_parts.append(scaled_values)\n","    scalers_dict[col_name] = {'scaler': scaler, 'log_applied': True, 'original_min': original_values.min(), 'original_max': original_values.max()}\n","    column_info_for_generator_output.append({'name': col_name, 'type': 'scaled_continuous_sigmoid'})\n","    print(f\"Preprocesado {col_name} con log1p + MinMaxScaler. Shape: {scaled_values.shape}\")\n","\n","# D. Otras columnas (MinMaxScaler para [0,1])\n","\n","other_cols = [col for col in original_columns if col not in [diabetes_col, gender_col] + special_cols_log_scale]\n","for col_name in other_cols:\n","    original_values = data_df[col_name].values.reshape(-1, 1)\n","    scaler = MinMaxScaler(feature_range=(0, 1))\n","    scaled_values = scaler.fit_transform(original_values)\n","    processed_data_parts.append(scaled_values)\n","    scalers_dict[col_name] = {'scaler': scaler, 'log_applied': False, 'original_min': original_values.min(), 'original_max': original_values.max()}\n","    column_info_for_generator_output.append({'name': col_name, 'type': 'scaled_continuous_sigmoid'})\n","    print(f\"Preprocesado {col_name} con MinMaxScaler. Shape: {scaled_values.shape}\")\n","\n","X_train_processed_np = np.concatenate(processed_data_parts, axis=1).astype(np.float32)\n","DATA_DIM = X_train_processed_np.shape[1]\n","print(f\"Forma final de los datos procesados (X_train_processed_np): {X_train_processed_np.shape}\")\n","\n","# Convertir datos a tensores de PyTorch\n","X_train_tensor = torch.tensor(X_train_processed_np, dtype=torch.float32).to(device)\n","\n","\n","# --- 3. Definir el modelo GAN (PyTorch) ---\n","\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim, data_dim):\n","        super(Generator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(latent_dim, 512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(0.4),\n","            nn.BatchNorm1d(512, momentum=0.8),\n","            nn.Linear(512, 1024),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(0.3),\n","            nn.BatchNorm1d(1024, momentum=0.8),\n","            nn.Linear(1024, 2048),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(2048, momentum=0.8),\n","            nn.Linear(2048, 4096),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(0.1),\n","            nn.BatchNorm1d(4096, momentum=0.8),\n","            nn.Linear(4096, data_dim),\n","            nn.Sigmoid() # Salida general en [0,1]\n","        )\n","\n","    def forward(self, z):\n","        return self.model(z)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, data_dim):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(data_dim, 1024),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(1024, 512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 1),\n","            nn.Sigmoid() # Salida binaria (real/falso)\n","        )\n","\n","    def forward(self, img):\n","        return self.model(img)\n","\n","# Inicializar generador y discriminador\n","generator = Generator(LATENT_DIM, DATA_DIM).to(device)\n","discriminator = Discriminator(DATA_DIM).to(device)\n","\n","# Load models\n","#generator.load_state_dict(torch.load(generator_path, map_location=device))\n","#discriminator.load_state_dict(torch.load(discriminator_path, map_location=device))\n","\n","# Función de pérdida\n","adversarial_loss = nn.BCELoss().to(device)\n","\n","# Optimizadores\n","optimizer_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE_G, betas=(BETA1, 0.999))\n","optimizer_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_D, betas=(BETA1, 0.999))\n","\n","print(\"\\n--- Arquitectura del Generador (PyTorch) ---\")\n","print(generator)\n","print(\"\\n--- Arquitectura del Discriminador (PyTorch) ---\")\n","print(discriminator)\n","\n","\n","# --- 4. Bucle de Entrenamiento (PyTorch) ---\n","print(\"\\nIniciando entrenamiento de la GAN con PyTorch...\")\n","d_loss_history = []\n","g_loss_history = []\n","d_acc_history = [] # Para la precisión del discriminador\n","\n","for epoch in range(EPOCHS):\n","    perm = torch.randperm(X_train_tensor.size(0))\n","    X_train_shuffled = X_train_tensor[perm]\n","\n","    d_loss_epoch = 0\n","    g_loss_epoch = 0\n","    num_batches = X_train_shuffled.size(0) // BATCH_SIZE\n","    for i in range(num_batches): # Iterar sobre batches\n","        # ---------------------\n","        #  Entrenar Discriminador\n","        # ---------------------\n","        discriminator.train()\n","        generator.eval() # Generador en modo evaluación para no actualizar sus BN stats aquí\n","\n","        # Datos reales\n","        real_imgs = X_train_shuffled[i*BATCH_SIZE:(i+1)*BATCH_SIZE].to(device)\n","        real_imgs += 0.1 * torch.randn_like(real_imgs)\n","        real_labels = torch.full((real_imgs.size(0), 1), 0.9, device=device)\n","        #real_labels = torch.ones(real_imgs.size(0), 1, dtype=torch.float32).to(device)\n","\n","        # Datos falsos\n","        noise = torch.randn(real_imgs.size(0), LATENT_DIM, dtype=torch.float32).to(device)\n","        fake_imgs = generator(noise)\n","        fake_imgs += 0.1 * torch.randn_like(fake_imgs)\n","        fake_labels = torch.full((fake_imgs.size(0), 1), 0.1, device=device)\n","        #fake_labels = torch.zeros(real_imgs.size(0), 1, dtype=torch.float32).to(device)\n","\n","        # Entrenar con datos reales\n","        optimizer_D.zero_grad()\n","        real_output = discriminator(real_imgs)\n","        d_loss_real = adversarial_loss(real_output, real_labels)\n","\n","        # Entrenar con datos falsos\n","        fake_output = discriminator(fake_imgs.detach()) # .detach() para no calcular gradientes para G aquí\n","        d_loss_fake = adversarial_loss(fake_output, fake_labels)\n","\n","        d_loss = (d_loss_real + d_loss_fake) / 2\n","        d_loss_epoch += d_loss.item()\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","        # Calcular precisión del discriminador (opcional)\n","        # d_accuracy = ((real_output > 0.5).float().sum() + (fake_output < 0.5).float().sum()) / (2 * real_imgs.size(0))\n","\n","\n","        # ---------------------\n","        #  Entrenar Generador\n","        # ---------------------\n","        for _ in range(4):\n","          generator.train() # Generador en modo entrenamiento\n","          optimizer_G.zero_grad()\n","\n","          # Generar datos falsos (nuevo batch de ruido)\n","          noise_g = torch.randn(BATCH_SIZE, LATENT_DIM, dtype=torch.float32).to(device) # Usar BATCH_SIZE fijo para G\n","          gen_imgs_for_g = generator(noise_g)\n","\n","          # Queremos que el discriminador piense que estos son reales\n","          # (usamos real_labels que son todos unos)\n","          # Asegurarse que real_labels_for_g tenga el tamaño correcto si BATCH_SIZE es diferente al último batch de D\n","          real_labels_for_g = torch.ones(gen_imgs_for_g.size(0), 1, dtype=torch.float32).to(device)\n","\n","          output_g = discriminator(gen_imgs_for_g)\n","          g_loss = adversarial_loss(output_g, real_labels_for_g)\n","          g_loss_epoch += g_loss.item()\n","          g_loss.backward()\n","          optimizer_G.step()\n","\n","    # Guardar el progreso al final de la época (promedio si se quiere)\n","    d_loss_history.append(d_loss_epoch / num_batches)\n","    g_loss_history.append(g_loss_epoch / (num_batches*4))\n","    # d_acc_history.append(d_accuracy.item())\n","\n","\n","    if (epoch + 1) % SAMPLE_INTERVAL == 0:\n","        # print(f\"{epoch + 1}/{EPOCHS} [D loss: {d_loss.item():.4f}, acc.: {d_accuracy.item()*100:.2f}%] [G loss: {g_loss.item():.4f}]\")\n","        print(f\"{epoch + 1}/{EPOCHS} [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n","        # Opcional: Generar y guardar una muestra de datos\n","        # generator.eval()\n","        # with torch.no_grad():\n","        #     noise_sample = torch.randn(5, LATENT_DIM, dtype=torch.float32).to(device)\n","        #     generated_sample_scaled = generator(noise_sample).cpu().numpy()\n","        #     print(\"Generated sample (scaled, PyTorch): \\n\", generated_sample_scaled[:2])\n","        # generator.train()\n","\n","# --- Guardar modelos entrenados ---\n","\n","torch.save(generator.state_dict(), generator_path)\n","torch.save(discriminator.state_dict(), discriminator_path)\n","\n","print(f\"Modelos guardados en '{generator_path}' y '{discriminator_path}'\")\n","\n","# --- Graficar historial de pérdidas ---\n","plt.figure(figsize=(10, 5))\n","plt.plot(d_loss_history, label='Discriminator Loss')\n","plt.plot(g_loss_history, label='Generator Loss')\n","plt.title(\"Historial de Pérdidas de la GAN (PyTorch)\")\n","plt.xlabel(\"Época\")\n","plt.ylabel(\"Pérdida\")\n","plt.legend()\n","plt.savefig(f'/content/gdrive/My Drive/ResultCSV/Multi/gan_loss_history_pytorch_{EPOCHS}_{iteration}.png')\n","plt.show()\n","\n","\n","# --- 5. Generación y Postprocesamiento de Datos Finales (PyTorch) ---\n","print(\"\\nGenerando datos sintéticos finales con PyTorch...\")\n","#num_samples_to_generate = 1000 # Esto determina la cantidad de datos sintéticos a generar broders\n","num_samples_per_class = 50000  # Number of samples you want for each class\n","num_classes = data_df['CLASS'].nunique()\n","total_samples_needed = num_samples_per_class * num_classes\n","extra_factor = 10\n","\n","generator.eval() # Modo evaluación\n","synthetic_df_final = pd.DataFrame()\n","all_generated_data_scaled = []\n","generation_batch_size = 512\n","\n","num_to_generate = total_samples_needed * extra_factor\n","\n","# Generar en batches si num_samples_to_generate es muy grande para evitar OOM\n","with torch.no_grad():\n","    for i in range(0, num_to_generate, generation_batch_size):\n","        current_batch_size = min(generation_batch_size, num_to_generate - i)\n","        if current_batch_size <= 0:\n","            break\n","        noise_final_batch = torch.randn(current_batch_size, LATENT_DIM, dtype=torch.float32).to(device)\n","        generated_batch_scaled = generator(noise_final_batch).cpu().numpy()\n","        all_generated_data_scaled.append(generated_batch_scaled)\n","\n","generated_data_scaled_final_np = np.concatenate(all_generated_data_scaled, axis=0)\n","\n","\n","current_col_idx_in_generated = 0\n","for col_info in column_info_for_generator_output:\n","    col_name = col_info['name']\n","    col_type = col_info['type']\n","\n","    if col_type == 'one_hot':\n","        num_classes_other = col_info['num_classes'] #originally num_classes\n","        one_hot_part = generated_data_scaled_final_np[:, current_col_idx_in_generated : current_col_idx_in_generated + num_classes_other]\n","        decoded_classes = np.argmax(one_hot_part, axis=1)\n","        synthetic_df_final[col_name] = decoded_classes\n","        # Si 'Gender' o 'CLASS', mapear de nuevo a sus etiquetas originales si es necesario para la visualización/guardado\n","        if col_name == 'CLASS':\n","            # Invertir el mapeo {'N': 0, 'P': 1, 'Y': 2}\n","            reverse_class_mapping = {v: k for k, v in class_mapping.items()}\n","            synthetic_df_final[col_name] = synthetic_df_final[col_name].map(reverse_class_mapping)\n","        elif col_name == 'Gender':\n","            # Invertir el mapeo {'F': 0, 'M': 1}\n","            reverse_gender_mapping = {0: 'F', 1: 'M'}\n","            synthetic_df_final[col_name] = synthetic_df_final[col_name].map(reverse_gender_mapping)\n","        current_col_idx_in_generated += num_classes_other\n","\n","    elif col_type == 'scaled_continuous_sigmoid':\n","        generated_values_scaled = generated_data_scaled_final_np[:, current_col_idx_in_generated : current_col_idx_in_generated + 1]\n","        current_col_idx_in_generated += 1\n","\n","        s_info = scalers_dict[col_name]\n","        scaler_obj = s_info['scaler']\n","        inverted_values = scaler_obj.inverse_transform(generated_values_scaled)\n","\n","        if s_info['log_applied']:\n","            inverted_values = np.expm1(inverted_values)\n","        if data_df[col_name].dtype == 'int64' or (data_df[col_name].dtype == 'float64' and np.all(data_df[col_name] == data_df[col_name].astype(int))):\n","            final_values = np.round(inverted_values)\n","        else:\n","            final_values = inverted_values\n","\n","        final_values = np.clip(final_values, s_info['original_min'], s_info['original_max'])\n","        synthetic_df_final[col_name] = final_values.flatten().astype(data_df[col_name].dtype)\n","\n","synthetic_df_final = synthetic_df_final[original_columns]\n","\n","balanced_synthetic = []\n","# Primero, asegúrate de que 'CLASS' esté en formato numérico (0, 1, 2) para poder usar range(num_classes)\n","# Si lo mapeaste de nuevo a 'N', 'P', 'Y' arriba, temporalmente conviértelo a numérico\n","temp_df_for_balancing = synthetic_df_final.copy()\n","temp_df_for_balancing['CLASS'] = temp_df_for_balancing['CLASS'].map(class_mapping)\n","\n","for class_value in range(num_classes):\n","    class_samples = temp_df_for_balancing[temp_df_for_balancing['CLASS'] == class_value]\n","    balanced_synthetic.append(class_samples.sample(n=num_samples_per_class, replace=True, random_state=42))\n","synthetic_df_final_balanced = pd.concat(balanced_synthetic).sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","# Mapear 'CLASS' de nuevo a 'N', 'P', 'Y' en el DataFrame final balanceado\n","synthetic_df_final_balanced['CLASS'] = synthetic_df_final_balanced['CLASS'].map(reverse_class_mapping)\n","\n","output_file_pytorch = f'/content/gdrive/My Drive/ResultCSV/Multi/generated_data_gan_pytorch_{EPOCHS}_{iteration}.csv'\n","synthetic_df_final_balanced.to_csv(output_file_pytorch, index=False)\n","print(f\"\\nDatos sintéticos generados y guardados en: {output_file_pytorch}\")\n","\n","# --- Mostrar algunas estadísticas de los datos generados (igual que antes) ---\n","print(\"\\n--- Descripción de los datos originales: ---\")\n","print(data_df.describe())\n","print(\"\\n--- Descripción de los datos sintéticos (PyTorch): ---\")\n","print(synthetic_df_final_balanced.describe())\n","\n","print(\"\\n--- Conteo de valores para CLASS (Original): ---\")\n","print(data_df['CLASS'].value_counts(normalize=True).sort_index())\n","print(\"\\n--- Conteo de valores para CLASS (Sintético - PyTorch): ---\")\n","print(synthetic_df_final_balanced['CLASS'].value_counts(normalize=True).sort_index())\n","\n","fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n","sns.histplot(data_df['AGE'], ax=axes[0], color='blue', label='Original', kde=True, stat=\"density\")\n","sns.histplot(synthetic_df_final_balanced['AGE'], ax=axes[0], color='green', label='Sintético (PyTorch)', kde=True, stat=\"density\")\n","axes[0].set_title('Distribución de AGE')\n","axes[0].legend()\n","\n","sns.histplot(data_df['BMI'], ax=axes[1], color='blue', label='Original', kde=True, stat=\"density\")\n","sns.histplot(synthetic_df_final_balanced['BMI'], ax=axes[1], color='green', label='Sintético (PyTorch)', kde=True, stat=\"density\")\n","axes[1].set_title('Distribución de BMI')\n","axes[1].legend()\n","\n","plt.tight_layout()\n","plt.savefig(f'/content/gdrive/My Drive/ResultCSV/Multi/generated_data_distributions_comparison_pytorch_{EPOCHS}_{iteration}.png')\n","plt.show()\n","\n","print(\"\\nFinalizado con PyTorch.\")"]},{"cell_type":"code","source":["# --- Función para comparar y visualizar distribuciones ---\n","def compare_data_distributions(real_df, generated_df, numerical_cols, categorical_cols, output_prefix=\"comparison\"):\n","    print(\"\\n--- Iniciando comparación de distribuciones (Reales vs. Sintéticas) ---\")\n","\n","    # 1. Comparación de Estadísticas Descriptivas Generales\n","    print(\"\\n--- Estadísticas Descriptivas - Datos Originales (después de SMOTE) ---\")\n","    print(real_df.describe())\n","    print(\"\\n--- Estadísticas Descriptivas - Datos Sintéticos ---\")\n","    print(generated_df.describe())\n","\n","    # 2. Comparación de Conteo de Valores (para Categóricas) y Varianzas (para Numéricas)\n","    print(\"\\n--- Conteo de Clases / Valores (Categóricas) ---\")\n","    for col in categorical_cols:\n","        print(f\"\\nColumna: {col}\")\n","        print(\"Real:\")\n","        print(real_df[col].value_counts(normalize=True).sort_index())\n","        print(\"Sintético:\")\n","        print(generated_df[col].value_counts(normalize=True).sort_index())\n","\n","    print(\"\\n--- Varianzas de Columnas Numéricas ---\")\n","    real_variances = real_df[numerical_cols].var()\n","    gen_variances = generated_df[numerical_cols].var()\n","    comparison_variances = pd.DataFrame({'Real_Variance': real_variances, 'Synthetic_Variance': gen_variances})\n","    print(comparison_variances)\n","\n","    # 3. Visualización de Histogramas/KDE (para Numéricas)\n","    print(\"\\n--- Visualizando distribuciones numéricas ---\")\n","    num_plots_per_row = 3\n","    num_rows_numerical = (len(numerical_cols) + num_plots_per_row - 1) // num_plots_per_row\n","    fig_num, axes_num = plt.subplots(num_rows_numerical, num_plots_per_row, figsize=(5 * num_plots_per_row, 4 * num_rows_numerical))\n","    axes_num = axes_num.flatten() # Para manejar subplots en 1D o 2D\n","\n","    for i, col in enumerate(numerical_cols):\n","        sns.histplot(real_df[col], ax=axes_num[i], color='blue', label='Real', kde=True, stat=\"density\", alpha=0.6)\n","        sns.histplot(generated_df[col], ax=axes_num[i], color='green', label='Sintético', kde=True, stat=\"density\", alpha=0.6)\n","        axes_num[i].set_title(f'Distribución de {col}')\n","        axes_num[i].legend()\n","\n","    # Ocultar ejes vacíos si hay menos subplots que el espacio total\n","    for j in range(i + 1, len(axes_num)):\n","        fig_num.delaxes(axes_num[j])\n","\n","    plt.tight_layout()\n","    plt.savefig(f\"{output_prefix}_numerical_distributions.png\")\n","    plt.show()\n","\n","    # 4. Visualización de Gráficos de Barras (para Categóricas)\n","    print(\"\\n--- Visualizando distribuciones categóricas ---\")\n","    num_plots_per_row = 2\n","    num_rows_categorical = (len(categorical_cols) + num_plots_per_row - 1) // num_plots_per_row\n","    fig_cat, axes_cat = plt.subplots(num_rows_categorical, num_plots_per_row, figsize=(6 * num_plots_per_row, 5 * num_rows_categorical))\n","    axes_cat = axes_cat.flatten()\n","\n","    for i, col in enumerate(categorical_cols):\n","        real_counts = real_df[col].value_counts(normalize=True).sort_index()\n","        gen_counts = generated_df[col].value_counts(normalize=True).sort_index()\n","\n","        # --- LÍNEA CORREGIDA AQUÍ ---\n","        # Usa .union() para obtener todas las categorías únicas de ambos Series de forma robusta\n","        all_categories = real_counts.index.union(gen_counts.index)\n","\n","        df_plot = pd.DataFrame({\n","            'Category': all_categories,\n","            'Real': real_counts.reindex(all_categories, fill_value=0),\n","            'Sintético': gen_counts.reindex(all_categories, fill_value=0)\n","        }).melt(id_vars='Category', var_name='Dataset', value_name='Proportion')\n","\n","        sns.barplot(x='Category', y='Proportion', hue='Dataset', data=df_plot, ax=axes_cat[i], palette={'Real': 'blue', 'Sintético': 'green'})\n","        axes_cat[i].set_title(f'Distribución de {col}')\n","        axes_cat[i].set_ylabel('Proporción')\n","\n","    for j in range(i + 1, len(axes_cat)):\n","        fig_cat.delaxes(axes_cat[j])\n","\n","    plt.tight_layout()\n","    plt.savefig(f\"{output_prefix}_categorical_distributions.png\")\n","    plt.show()\n","\n","    print(\"\\n--- Fin de la comparación de distribuciones ---\")\n","\n","\n","# --- Dónde llamar la función de comparación en tu script ---\n","# Asegúrate de que estas variables estén definidas y contengan tus DataFrames finales:\n","# data_df (tu DataFrame de datos reales/SMOTEd y limpios)\n","# synthetic_df_final_balanced (tu DataFrame de datos generados por la GAN)\n","\n","# Define tus columnas numéricas y categóricas\n","numerical_features_for_comparison = ['AGE', 'Urea', 'Cr', 'HbA1c', 'Chol', 'TG', 'HDL', 'LDL', 'VLDL', 'BMI']\n","categorical_features_for_comparison = ['Gender', 'CLASS']\n","\n","# Llama a la función de comparación\n","compare_data_distributions(\n","    real_df=data_df, # Este es tu DF de datos originales después de SMOTE y limpieza\n","    generated_df=synthetic_df_final_balanced, # Este es tu DF de datos generados por la GAN\n","    numerical_cols=numerical_features_for_comparison,\n","    categorical_cols=categorical_features_for_comparison,\n","    output_prefix=\"gan_data_comparison\" # Prefijo para los nombres de los archivos de imagen\n",")"],"metadata":{"id":"TLKz4yruCDHt","executionInfo":{"status":"aborted","timestamp":1751576760015,"user_tz":240,"elapsed":7,"user":{"displayName":"Sebastián Ignacio García Péndola","userId":"08175071328414149400"}}},"id":"TLKz4yruCDHt","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}