{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56e94ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n",
      "Iniciando preprocesamiento...\n",
      "Preprocesado Diabetes_012 con One-Hot Encoding. Shape: (253680, 3)\n",
      "Preprocesado MentHlth con log1p + MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado PhysHlth con log1p + MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado HighBP con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado HighChol con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado CholCheck con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado BMI con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado Smoker con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado Stroke con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado HeartDiseaseorAttack con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado PhysActivity con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado Fruits con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado Veggies con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado HvyAlcoholConsump con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado AnyHealthcare con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado NoDocbcCost con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado GenHlth con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado DiffWalk con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado Sex con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado Age con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado Education con MinMaxScaler. Shape: (253680, 1)\n",
      "Preprocesado Income con MinMaxScaler. Shape: (253680, 1)\n",
      "Forma final de los datos procesados (X_train_processed_np): (253680, 24)\n",
      "\n",
      "--- Arquitectura del Generador (PyTorch) ---\n",
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): BatchNorm1d(1024, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n",
      "    (9): Linear(in_features=1024, out_features=24, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "--- Arquitectura del Discriminador (PyTorch) ---\n",
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Dropout(p=0.4, inplace=False)\n",
      "    (5): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (6): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "Iniciando entrenamiento de la GAN con PyTorch...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 192\u001b[0m\n\u001b[0;32m    189\u001b[0m     output_g \u001b[38;5;241m=\u001b[39m discriminator(gen_imgs_for_g)\n\u001b[0;32m    190\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(output_g, real_labels_for_g)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mg_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     optimizer_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Guardar el progreso al final de la época (promedio si se quiere)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split # No se usa directamente para GAN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset # No se usará DataLoader para este ejemplo simple\n",
    "\n",
    "# --- 0. Parámetros de la GAN y del Entrenamiento ---\n",
    "LATENT_DIM = 100\n",
    "EPOCHS = 10 # Puede necesitar muchas más (ej. 10000-50000+) y ajustes\n",
    "BATCH_SIZE = 64\n",
    "SAMPLE_INTERVAL = 1 # Cada cuántas épocas guardar una muestra de datos generados\n",
    "LEARNING_RATE_G = 0.0002\n",
    "LEARNING_RATE_D = 0.0002\n",
    "BETA1 = 0.5 # Parámetro de Adam\n",
    "\n",
    "# Configurar dispositivo (GPU si está disponible)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# --- 1. Cargar el archivo CSV original ---\n",
    "input_file = \"Datasets/diabetes_012_health_indicators_BRFSS2015.csv\"\n",
    "data_df = pd.read_csv(input_file)\n",
    "original_columns = data_df.columns.tolist()\n",
    "\n",
    "# --- 2. Preprocesamiento (Igual que antes, usando scikit-learn y numpy) ---\n",
    "print(\"Iniciando preprocesamiento...\")\n",
    "processed_data_parts = []\n",
    "scalers_dict = {}\n",
    "column_info_for_generator_output = []\n",
    "\n",
    "special_cols_log_scale = ['MentHlth', 'PhysHlth']\n",
    "diabetes_col = 'Diabetes_012'\n",
    "\n",
    "# A. Diabetes_012 (One-Hot Encoding)\n",
    "#    Para PyTorch, no necesitamos to_categorical de Keras, podemos hacerlo con numpy\n",
    "num_classes_diabetes = 3\n",
    "diabetes_one_hot = np.eye(num_classes_diabetes)[data_df[diabetes_col].astype(int)]\n",
    "processed_data_parts.append(diabetes_one_hot)\n",
    "column_info_for_generator_output.append({'name': diabetes_col, 'type': 'one_hot', 'num_classes': num_classes_diabetes})\n",
    "print(f\"Preprocesado {diabetes_col} con One-Hot Encoding. Shape: {diabetes_one_hot.shape}\")\n",
    "\n",
    "# B. MentHlth y PhysHlth (log1p + MinMaxScaler)\n",
    "for col_name in special_cols_log_scale:\n",
    "    original_values = data_df[col_name].values.reshape(-1, 1)\n",
    "    log_transformed_values = np.log1p(original_values)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_values = scaler.fit_transform(log_transformed_values)\n",
    "    processed_data_parts.append(scaled_values)\n",
    "    scalers_dict[col_name] = {'scaler': scaler, 'log_applied': True, 'original_min': original_values.min(), 'original_max': original_values.max()}\n",
    "    column_info_for_generator_output.append({'name': col_name, 'type': 'scaled_continuous_sigmoid'})\n",
    "    print(f\"Preprocesado {col_name} con log1p + MinMaxScaler. Shape: {scaled_values.shape}\")\n",
    "\n",
    "# C. Otras columnas (MinMaxScaler para [0,1])\n",
    "other_cols = [col for col in original_columns if col not in [diabetes_col] + special_cols_log_scale]\n",
    "for col_name in other_cols:\n",
    "    original_values = data_df[col_name].values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_values = scaler.fit_transform(original_values)\n",
    "    processed_data_parts.append(scaled_values)\n",
    "    scalers_dict[col_name] = {'scaler': scaler, 'log_applied': False, 'original_min': original_values.min(), 'original_max': original_values.max()}\n",
    "    column_info_for_generator_output.append({'name': col_name, 'type': 'scaled_continuous_sigmoid'})\n",
    "    print(f\"Preprocesado {col_name} con MinMaxScaler. Shape: {scaled_values.shape}\")\n",
    "\n",
    "X_train_processed_np = np.concatenate(processed_data_parts, axis=1).astype(np.float32)\n",
    "DATA_DIM = X_train_processed_np.shape[1]\n",
    "print(f\"Forma final de los datos procesados (X_train_processed_np): {X_train_processed_np.shape}\")\n",
    "\n",
    "# Convertir datos a tensores de PyTorch\n",
    "X_train_tensor = torch.tensor(X_train_processed_np, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "# --- 3. Definir el modelo GAN (PyTorch) ---\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, data_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm1d(256, momentum=0.8),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm1d(512, momentum=0.8),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm1d(1024, momentum=0.8),\n",
    "            nn.Linear(1024, data_dim),\n",
    "            nn.Sigmoid() # Salida general en [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid() # Salida binaria (real/falso)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "# Inicializar generador y discriminador\n",
    "generator = Generator(LATENT_DIM, DATA_DIM).to(device)\n",
    "discriminator = Discriminator(DATA_DIM).to(device)\n",
    "\n",
    "# Función de pérdida\n",
    "adversarial_loss = nn.BCELoss().to(device)\n",
    "\n",
    "# Optimizadores\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE_G, betas=(BETA1, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_D, betas=(BETA1, 0.999))\n",
    "\n",
    "print(\"\\n--- Arquitectura del Generador (PyTorch) ---\")\n",
    "print(generator)\n",
    "print(\"\\n--- Arquitectura del Discriminador (PyTorch) ---\")\n",
    "print(discriminator)\n",
    "\n",
    "\n",
    "# --- 4. Bucle de Entrenamiento (PyTorch) ---\n",
    "print(\"\\nIniciando entrenamiento de la GAN con PyTorch...\")\n",
    "d_loss_history = []\n",
    "g_loss_history = []\n",
    "d_acc_history = [] # Para la precisión del discriminador\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(X_train_tensor.size(0) // BATCH_SIZE): # Iterar sobre batches\n",
    "        # ---------------------\n",
    "        #  Entrenar Discriminador\n",
    "        # ---------------------\n",
    "        discriminator.train()\n",
    "        generator.eval() # Generador en modo evaluación para no actualizar sus BN stats aquí\n",
    "\n",
    "        # Datos reales\n",
    "        real_imgs = X_train_tensor[i*BATCH_SIZE:(i+1)*BATCH_SIZE].to(device)\n",
    "        real_labels = torch.ones(real_imgs.size(0), 1, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Datos falsos\n",
    "        noise = torch.randn(real_imgs.size(0), LATENT_DIM, dtype=torch.float32).to(device)\n",
    "        fake_imgs = generator(noise)\n",
    "        fake_labels = torch.zeros(real_imgs.size(0), 1, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Entrenar con datos reales\n",
    "        optimizer_D.zero_grad()\n",
    "        real_output = discriminator(real_imgs)\n",
    "        d_loss_real = adversarial_loss(real_output, real_labels)\n",
    "        \n",
    "        # Entrenar con datos falsos\n",
    "        fake_output = discriminator(fake_imgs.detach()) # .detach() para no calcular gradientes para G aquí\n",
    "        d_loss_fake = adversarial_loss(fake_output, fake_labels)\n",
    "        \n",
    "        d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Calcular precisión del discriminador (opcional)\n",
    "        # d_accuracy = ((real_output > 0.5).float().sum() + (fake_output < 0.5).float().sum()) / (2 * real_imgs.size(0))\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Entrenar Generador\n",
    "        # ---------------------\n",
    "        generator.train() # Generador en modo entrenamiento\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generar datos falsos (nuevo batch de ruido)\n",
    "        noise_g = torch.randn(BATCH_SIZE, LATENT_DIM, dtype=torch.float32).to(device) # Usar BATCH_SIZE fijo para G\n",
    "        gen_imgs_for_g = generator(noise_g)\n",
    "        \n",
    "        # Queremos que el discriminador piense que estos son reales\n",
    "        # (usamos real_labels que son todos unos)\n",
    "        # Asegurarse que real_labels_for_g tenga el tamaño correcto si BATCH_SIZE es diferente al último batch de D\n",
    "        real_labels_for_g = torch.ones(gen_imgs_for_g.size(0), 1, dtype=torch.float32).to(device)\n",
    "        \n",
    "        output_g = discriminator(gen_imgs_for_g)\n",
    "        g_loss = adversarial_loss(output_g, real_labels_for_g)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    # Guardar el progreso al final de la época (promedio si se quiere)\n",
    "    d_loss_history.append(d_loss.item())\n",
    "    g_loss_history.append(g_loss.item())\n",
    "    # d_acc_history.append(d_accuracy.item())\n",
    "\n",
    "\n",
    "    if (epoch + 1) % SAMPLE_INTERVAL == 0:\n",
    "        # print(f\"{epoch + 1}/{EPOCHS} [D loss: {d_loss.item():.4f}, acc.: {d_accuracy.item()*100:.2f}%] [G loss: {g_loss.item():.4f}]\")\n",
    "        print(f\"{epoch + 1}/{EPOCHS} [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "        # Opcional: Generar y guardar una muestra de datos\n",
    "        # generator.eval()\n",
    "        # with torch.no_grad():\n",
    "        #     noise_sample = torch.randn(5, LATENT_DIM, dtype=torch.float32).to(device)\n",
    "        #     generated_sample_scaled = generator(noise_sample).cpu().numpy()\n",
    "        #     print(\"Generated sample (scaled, PyTorch): \\n\", generated_sample_scaled[:2])\n",
    "        # generator.train()\n",
    "\n",
    "\n",
    "# --- Graficar historial de pérdidas ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(d_loss_history, label='Discriminator Loss')\n",
    "plt.plot(g_loss_history, label='Generator Loss')\n",
    "plt.title(\"Historial de Pérdidas de la GAN (PyTorch)\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.legend()\n",
    "plt.savefig(\"gan_loss_history_pytorch.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 5. Generación y Postprocesamiento de Datos Finales (PyTorch) ---\n",
    "print(\"\\nGenerando datos sintéticos finales con PyTorch...\")\n",
    "num_samples_to_generate = 1000 # Esto determina la cantidad de datos sintéticos a generar broders\n",
    "generator.eval() # Modo evaluación\n",
    "synthetic_df_final = pd.DataFrame()\n",
    "\n",
    "# Generar en batches si num_samples_to_generate es muy grande para evitar OOM\n",
    "all_generated_data_scaled = []\n",
    "generation_batch_size = 512 # Puede ajustarse\n",
    "with torch.no_grad(): # No necesitamos gradientes para la generación\n",
    "    for i in range(0, num_samples_to_generate, generation_batch_size):\n",
    "        current_batch_size = min(generation_batch_size, num_samples_to_generate - i)\n",
    "        if current_batch_size == 0: break\n",
    "        noise_final_batch = torch.randn(current_batch_size, LATENT_DIM, dtype=torch.float32).to(device)\n",
    "        generated_batch_scaled = generator(noise_final_batch).cpu().numpy()\n",
    "        all_generated_data_scaled.append(generated_batch_scaled)\n",
    "\n",
    "generated_data_scaled_final_np = np.concatenate(all_generated_data_scaled, axis=0)\n",
    "\n",
    "\n",
    "current_col_idx_in_generated = 0\n",
    "for col_info in column_info_for_generator_output:\n",
    "    col_name = col_info['name']\n",
    "    col_type = col_info['type']\n",
    "\n",
    "    if col_type == 'one_hot':\n",
    "        num_classes = col_info['num_classes']\n",
    "        one_hot_part = generated_data_scaled_final_np[:, current_col_idx_in_generated : current_col_idx_in_generated + num_classes]\n",
    "        synthetic_df_final[col_name] = np.argmax(one_hot_part, axis=1)\n",
    "        current_col_idx_in_generated += num_classes\n",
    "    \n",
    "    elif col_type == 'scaled_continuous_sigmoid':\n",
    "        generated_values_scaled = generated_data_scaled_final_np[:, current_col_idx_in_generated : current_col_idx_in_generated + 1]\n",
    "        current_col_idx_in_generated += 1\n",
    "        \n",
    "        s_info = scalers_dict[col_name]\n",
    "        scaler_obj = s_info['scaler']\n",
    "        inverted_values = scaler_obj.inverse_transform(generated_values_scaled)\n",
    "        \n",
    "        if s_info['log_applied']:\n",
    "            inverted_values = np.expm1(inverted_values)\n",
    "            \n",
    "        if data_df[col_name].dtype == 'int64' or data_df[col_name].dtype == 'float64' and np.all(data_df[col_name] == data_df[col_name].astype(int)):\n",
    "             final_values = np.round(inverted_values)\n",
    "        else:\n",
    "            final_values = inverted_values\n",
    "\n",
    "        final_values = np.clip(final_values, s_info['original_min'], s_info['original_max'])\n",
    "        synthetic_df_final[col_name] = final_values.flatten().astype(data_df[col_name].dtype)\n",
    "\n",
    "synthetic_df_final = synthetic_df_final[original_columns]\n",
    "output_file_pytorch = \"Datasets/generated_data_gan_pytorch.csv\"\n",
    "synthetic_df_final.to_csv(output_file_pytorch, index=False)\n",
    "print(f\"\\nDatos sintéticos generados y guardados en: {output_file_pytorch}\")\n",
    "\n",
    "# --- Mostrar algunas estadísticas de los datos generados (igual que antes) ---\n",
    "print(\"\\n--- Descripción de los datos originales: ---\")\n",
    "print(data_df.describe())\n",
    "print(\"\\n--- Descripción de los datos sintéticos (PyTorch): ---\")\n",
    "print(synthetic_df_final.describe())\n",
    "\n",
    "print(\"\\n--- Conteo de valores para Diabetes_012 (Original): ---\")\n",
    "print(data_df['Diabetes_012'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\n--- Conteo de valores para Diabetes_012 (Sintético - PyTorch): ---\")\n",
    "print(synthetic_df_final['Diabetes_012'].value_counts(normalize=True).sort_index())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.histplot(data_df['MentHlth'], ax=axes[0], color='blue', label='Original', kde=True, stat=\"density\")\n",
    "sns.histplot(synthetic_df_final['MentHlth'], ax=axes[0], color='green', label='Sintético (PyTorch)', kde=True, stat=\"density\")\n",
    "axes[0].set_title('Distribución de MentHlth')\n",
    "axes[0].legend()\n",
    "\n",
    "sns.histplot(data_df['PhysHlth'], ax=axes[1], color='blue', label='Original', kde=True, stat=\"density\")\n",
    "sns.histplot(synthetic_df_final['PhysHlth'], ax=axes[1], color='green', label='Sintético (PyTorch)', kde=True, stat=\"density\")\n",
    "axes[1].set_title('Distribución de PhysHlth')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"generated_data_distributions_comparison_pytorch.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinalizado con PyTorch.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
