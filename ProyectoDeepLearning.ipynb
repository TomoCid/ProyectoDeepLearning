{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95599d36",
   "metadata": {},
   "source": [
    "# MLP Binario con dataset real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddadc8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejo de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Modelos de Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "#Preprocesamiento de datos\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Gráficos\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configuración warnings (Quitar en caso de errores desconocidos)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Versión de paquetes usados\n",
    "color = '\\033[1m\\033[38;5;208m'\n",
    "print(f\"{color}Versión de las librerias utilizadas:\")\n",
    "print(f\"{color}- Version torch: {torch.__version__}\")\n",
    "print(f\"{color}- Version pandas: {pd.__version__}\")\n",
    "print(f\"{color}- Version numpy: {np.__version__}\")\n",
    "print(f\"{color}- Version sklearn: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5126a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numero total de datos a usar de los 100k de datos\n",
    "num_datos = 1001 #1k, 5k, 10k, 50k, 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7119d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar Dataset\n",
    "try:\n",
    "    data = pd.read_csv(f'/content/gdrive/My Drive/G11/Datasets/dataset_original.csv')\n",
    "    print(\"Archivo cargado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir variables categóricas a numéricas\n",
    "data = pd.get_dummies(data, columns=['gender', 'smoking_history'], drop_first=True)\n",
    "\n",
    "# Eliminar filas con valores nulos si los hubiera\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Renombrar columnas para que no tengan caracteres especiales\n",
    "data.columns = data.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "\n",
    "print(\"Preprocesamiento listo. Nuevas columnas:\")\n",
    "print(data.columns)\n",
    "data.head()\n",
    "data_sample = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c0038",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 110425\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b38f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resample manteniendo proporcion de datos original\n",
    "class_0 = data[data['diabetes'] == 0].sample(n=int(num_datos*0.915), random_state=42) \n",
    "class_1 = data[data['diabetes'] == 1].sample(n=int(num_datos*0.085), random_state=42) \n",
    "balanced_train_data = pd.concat([class_0, class_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_sample = balanced_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Separar características (x) y objetivo (y) de la muestra\n",
    "x = data_sample.drop(columns=[\"diabetes\"]).to_numpy()\n",
    "y = data_sample[\"diabetes\"].to_numpy()\n",
    "\n",
    "# 2. Dividir en conjuntos de entrenamiento, validación y prueba \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42, stratify=y) # 20% para test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42, stratify=y_train) # 15% del resto para validación\n",
    "\n",
    "# 3. Aplicar el escalado\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val) # Usar el mismo scaler del train\n",
    "X_test = scaler.transform(X_test) # Usar el mismo scaler del train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf00b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicar Tensores\n",
    "#Train\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "#Val\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "#Test\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir los Batchs del dataset\n",
    "batchsize = 32\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_data = DiabetesDataset(X_train, y_train)\n",
    "val_data = DiabetesDataset(X_val, y_val)\n",
    "test_data = DiabetesDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81802f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir el Modelo a usar\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hc1 = nn.Linear(x.shape[1], 256) # Ajuste automático al número de columnas\n",
    "        self.hc2 = nn.Linear(256, 128)\n",
    "        self.hc3 = nn.Linear(128, 64)\n",
    "        self.hc4 = nn.Linear(64, 2)\n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.dp1 = nn.Dropout(0.4)\n",
    "        self.dp2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        l1 = self.dp1(self.act(self.hc1(input)))\n",
    "        l2 = self.dp2(self.act(self.hc2(l1)))\n",
    "        l3 = self.act(self.hc3(l2))\n",
    "        output = self.hc4(l3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bded0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa el Modelo, define el Learning Rate y el optimizador\n",
    "testeo = MLP()\n",
    "lr = 1e-4\n",
    "opt = torch.optim.Adam(testeo.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "#Calcula peso para las clases para posteriormente darle mas importancia las clases minoritaria\n",
    "class_counts = Counter(y)\n",
    "print(class_counts)\n",
    "total_samples = len(y)\n",
    "num_classes = len(class_counts)\n",
    "class_weights = [0.0] * num_classes\n",
    "for class_id, count in class_counts.items():\n",
    "    class_weights[class_id] = total_samples / count\n",
    "print(\"\\nPesos iniciales calculados (más alto para clases raras):\")\n",
    "print(class_weights)\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(f\"\\nTensor de pesos para CrossEntropyLoss: {weights_tensor}\")\n",
    "\n",
    "#Se define loss con los weight previamente calculados\n",
    "loss_func = nn.CrossEntropyLoss(weight=weights_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e5efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direccion donde estan o se guardarn los pesos y el modelo\n",
    "testeo_path = f'/content/gdrive/My Drive/G11/Guardar_modelo/MLP/Binary_original/Binary_original_{num_datos}_pytorch.pth'\n",
    "testeo_path_FULL = f'/content/gdrive/My Drive/G11/Guardar_modelo/MLP/Binary_original/Binary_original_{num_datos}_pytorch_FULL.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae72cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar Pesos\n",
    "#testeo.load_state_dict(torch.load(testeo_path))\n",
    "# Cargar Modelo\n",
    "#testeo = torch.load(testeo_path_FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamiento\n",
    "\n",
    "n_epochs = 2000\n",
    "\n",
    "history = {\n",
    "    \"TL\" : [],\n",
    "    \"VL\" : []\n",
    "}\n",
    "\n",
    "early_stopping = {\n",
    "    \"delta\" : 1e-5,\n",
    "    \"patience\": 10\n",
    "}\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "aux = 0\n",
    "\n",
    "for i in range(n_epochs+1):\n",
    "    testeo.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:  \n",
    "      output = testeo(X_batch)\n",
    "      loss = loss_func(output, y_batch) \n",
    "      \n",
    "      opt.zero_grad()\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    history[\"TL\"].append(epoch_loss)\n",
    "\n",
    "    testeo.eval()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in val_loader:\n",
    "      with torch.no_grad():\n",
    "        preds_val = testeo(X_batch)\n",
    "      val_loss = loss_func(preds_val, y_batch)\n",
    "\n",
    "      epoch_loss += val_loss.item()\n",
    "    epoch_loss /= len(val_loader)\n",
    "    history[\"VL\"].append(epoch_loss)\n",
    "    \n",
    "    if epoch_loss + early_stopping[\"delta\"] < best_val_loss:\n",
    "      best_val_loss = epoch_loss\n",
    "      aux = 0\n",
    "    else:\n",
    "      aux += 1\n",
    "      if aux >= early_stopping[\"patience\"]:\n",
    "        print(f\"Terminando el entrenamiento en la época {i}\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Pesos\n",
    "torch.save(testeo.state_dict(), testeo_path)\n",
    "#Guardar Modelo Completo\n",
    "torch.save(testeo, testeo_path_FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0129300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafico de loss en tre training y validacion\n",
    "plt.plot(history[\"TL\"], label=\"Train\")\n",
    "plt.plot(history[\"VL\"], label=\"Val\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo de accuracy del cnjunto de entrenamiento y de testeo\n",
    "#Train \n",
    "with torch.no_grad():\n",
    "    output_train = testeo(X_train)\n",
    "y_hat_train = output_train.numpy()\n",
    "y_hat_train = [np.argmax(i) for i in y_hat_train]\n",
    "print(f'Training Accuracy: {accuracy_score(y_train, y_hat_train):.2f}')\n",
    "#Testeo \n",
    "with torch.no_grad():\n",
    "    output = testeo(X_test)  \n",
    "y_hat = output.numpy()\n",
    "y_hat = [np.argmax(i) for i in y_hat]\n",
    "print(f'Testing Accuracy: {accuracy_score(y_test, y_hat):.2f}')\n",
    "\n",
    "#precision, recall f1-score support usando resultados de conjunto de testeo\n",
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153fd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm =confusion_matrix(y_test, y_hat)\n",
    "cm_plot = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "cm_plot.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43175e",
   "metadata": {},
   "source": [
    "# XGBoost(ML) Binario con dataset real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7496e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports necesarios para el análisis de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Librerías para visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#XGBoost para el modelo de clasificación\n",
    "import xgboost as xgb\n",
    "\n",
    "#Herramientas de sklearn para entrenamiento y evaluación\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "\n",
    "#Se usa para guardar el modelo entrenado\n",
    "import joblib\n",
    "\n",
    "#Suprimir warnings para una salida más limpia\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff2178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar el dataset de diabetes\n",
    "data = pd.read_csv('/content/gdrive/My Drive/G11/Datasets/dataset_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpiar y estandarizar la columna de género\n",
    "data['gender'] = data['gender'].astype(str).str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostrar información general del dataset para comprobar visualmente\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3690be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se recategoriza el historial de tabaquismo en grupos más simples\n",
    "def recategorize_smoking(smoking_status):\n",
    "    if smoking_status in ['never', 'No Info']:\n",
    "        return 'non-smoker'\n",
    "    elif smoking_status == 'current':\n",
    "        return 'current'\n",
    "    elif smoking_status in ['ever', 'former', 'not current']:\n",
    "        return 'past_smoker'\n",
    "    \n",
    "data['smoking_history'] = data['smoking_history'].apply(recategorize_smoking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62770fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuración del tamaño de muestra para el entrenamiento\n",
    "total_samples = 5000  \n",
    "\n",
    "#Calcular muestras por clase manteniendo proporciones 91.5% / 8.5%\n",
    "class_0_samples = int(total_samples * 0.915)\n",
    "class_1_samples = int(total_samples * 0.085)\n",
    "\n",
    "#print(f\"=== CONFIGURACIÓN: {total_samples} muestras totales ===\")\n",
    "#print(f\"Clase 0 (No diabetes): {class_0_samples}\")\n",
    "#print(f\"Clase 1 (Diabetes): {class_1_samples}\")\n",
    "\n",
    "#Crear muestra manteniendo las proporciones originales\n",
    "stratified_data = pd.concat([\n",
    "    data[data['diabetes'] == 0].sample(n=min(class_0_samples, len(data[data['diabetes'] == 0])), random_state=42),\n",
    "    data[data['diabetes'] == 1].sample(n=min(class_1_samples, len(data[data['diabetes'] == 1])), random_state=42)\n",
    "]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#print(f\"\\nDistribución final:\")\n",
    "#print(stratified_data['diabetes'].value_counts())\n",
    "#print(f\"Proporciones finales:\")\n",
    "#print(stratified_data['diabetes'].value_counts(normalize=True))\n",
    "\n",
    "data = stratified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fcd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#En esta celda se configura el preprocesamiento de datos\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #Primero se estandarizan las variables numéricas\n",
    "        ('num', StandardScaler(), ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level','hypertension','heart_disease']),\n",
    "        #Luego se codificar las variables categóricas\n",
    "        ('cat', OneHotEncoder(), ['gender','smoking_history'])\n",
    "    ])\n",
    "\n",
    "X = data.drop('diabetes', axis=1)\n",
    "y = data['diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular pesos automáticamente para balancear clases desbalanceadas\n",
    "class_counts = Counter(y)\n",
    "#print(f\"Conteo de clases: {class_counts}\")\n",
    "\n",
    "total_samples = len(y)\n",
    "weight_for_0 = total_samples / class_counts[0] \n",
    "weight_for_1 = total_samples / class_counts[1]\n",
    "\n",
    "#En XGBoost, scale_pos_weight es el ratio que penaliza más los errores en la clase minoritaria, esto se usa porque se tienen muy pocos datos de pacientes con diabetes\n",
    "scale_pos_weight = weight_for_1 / weight_for_0\n",
    "#print(f\"Scale pos weight calculado: {scale_pos_weight}\")\n",
    "\n",
    "#Se crea el pipeline que incluye preprocesamiento y modelo con pesos balanceados\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', xgb.XGBClassifier(scale_pos_weight=scale_pos_weight))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcf0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se definen los hiperparámetros para optimizar con GridSearch\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100],\n",
    "    'classifier__max_depth': [3],\n",
    "    'classifier__learning_rate': [0.001, 0.01],\n",
    "    'classifier__subsample': [0.8],\n",
    "    'classifier__colsample_bytree': [0.8]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b70748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se configura la búsqueda de hiperparámetros usando Cross-Validation\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "\n",
    "#Aquí se dividen los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Y finalmente se entrena el modelo con los mejores hiperparámetros\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#print(\"Best Parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a20e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta celda permite guardar el modelo entrenado\n",
    "model_dir = f'/content/gdrive/My Drive/G11/Guardar_modelo/ML/Binary/'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model_path = f'{model_dir}Binary_{total_samples}_xgboost.pkl'\n",
    "joblib.dump(grid_search.best_estimator_, model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb02f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se hacen predicciones en el conjunto de prueba para evaluar el rendimiento del modelo\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print(\"Model Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Finalmente se crea y se muestra la matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0577ea12",
   "metadata": {},
   "source": [
    "# TabNet Binario con dataset real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad383f",
   "metadata": {},
   "source": [
    "Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac7035",
   "metadata": {},
   "source": [
    "Ajustamos el número de muestras (El máximo del dataset es 100.000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Samples = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089f016",
   "metadata": {},
   "source": [
    "Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e49249",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv(f'/content/gdrive/My Drive/G11/Datasets/dataset_original.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No se encontró el archivo 'dataset_original.csv'.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b189b760",
   "metadata": {},
   "source": [
    "Codificar las features categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'diabetes'\n",
    "categorical_features = ['gender', 'smoking_history']\n",
    "numerical_features = [col for col in data.columns if col not in categorical_features + [target_column]]\n",
    "# Codificar las características categóricas\n",
    "label_encoders_X = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = data[col].astype(str)\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders_X[col] = le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e7032",
   "metadata": {},
   "source": [
    "Ajustar el número de muestras con la misma proporción entre clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_class_count = data['diabetes'].value_counts().min()\n",
    "if N_Samples > len(data) or int(N_Samples * data['diabetes'].value_counts(normalize=True).min()) > min_class_count:\n",
    "     print(f\"Advertencia: El tamaño de muestra es muy grande. Se usará el dataset completo.\")\n",
    "     data = data.copy()\n",
    "else:\n",
    "    proportions = data['diabetes'].value_counts(normalize=True)\n",
    "    data = data.groupby('diabetes', group_keys=False).apply(\n",
    "        lambda x: x.sample(int(N_Samples * proportions[x.name]), random_state=42)\n",
    "    )\n",
    "\n",
    "print(f\"Cantidad de datos a considerar: {len(data)}\")\n",
    "print(\"\\nProporción de clases en el nuevo dataset:\")\n",
    "print(data['diabetes'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46372bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[target_column] = data[target_column].astype(int)\n",
    "\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Se usa un 80% para entrenamiento y un 20% para test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Escalado de las características numéricas\n",
    "scaler = StandardScaler()\n",
    "X_train.loc[:, numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "X_train_np = X_train.values\n",
    "y_train_np = y_train.values\n",
    "X_test_np = X_test.values\n",
    "y_test_np = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29973aa9",
   "metadata": {},
   "source": [
    "## Saltar esta celda en caso de querer usar el modelo guardado -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febdda7",
   "metadata": {},
   "source": [
    "Preparación de parametros, inicialización y entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requisisto para TabNet: Busca los índices y dimensiones de las características categóricas\n",
    "cat_idxs = [X.columns.get_loc(col) for col in categorical_features]\n",
    "# Requisito para TabNet: Calcula cuántas categorías únicas hay en cada columna categórica\n",
    "cat_dims = [len(le.classes_) for le in label_encoders_X.values()]\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "weighted_loss_fn = torch.nn.CrossEntropyLoss() # Función de pérdida\n",
    "\n",
    "# Inicialización del modelo TabNetClassifier\n",
    "clf = TabNetClassifier(\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    optimizer_params=dict(lr=1e-3), # Ajuste del learning rate\n",
    "    optimizer_fn=torch.optim.Adam, # Se usa el optimizador Adam\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo TabNetClassifier\n",
    "print(f\"\\n Iniciando Entrenamiento de TabNet:{' ' + DEVICE.upper()}\")\n",
    "clf.fit(\n",
    "    X_train=X_train_np, y_train=y_train_np,\n",
    "    eval_set=[(X_test_np, y_test_np)],\n",
    "    patience=10, # Early stopping después de 10 epoch sin mejora\n",
    "    max_epochs=100,\n",
    "    eval_metric=['accuracy', 'logloss'], # Métricas de evaluación\n",
    "    loss_fn=weighted_loss_fn, # Función de pérdida\n",
    "    batch_size=256 # Número de muestras por cada paso de entrenamiento\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = os.getcwd()\n",
    "carpeta = \"/content/gdrive/My Drive/G11/Guardar_modelo/TabNet\"\n",
    "archivo_xgb = f'Binary_TabNet_{N_Samples}'\n",
    "\n",
    "ruta = os.path.join(directorio, carpeta, archivo_xgb)\n",
    "\n",
    "directorio_final_para_guardar = os.path.dirname(ruta)\n",
    "os.makedirs(directorio_final_para_guardar, exist_ok=True)\n",
    "\n",
    "clf.save_model(ruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81562238",
   "metadata": {},
   "source": [
    "Se puede cargar el modelo descomentando las primeras líneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga del modelo guardado\n",
    "#preTrained = TabNetClassifier()\n",
    "#preTrained.load_model(ruta)\n",
    "\n",
    "y_pred = clf.predict(X_test_np) #Cambiar cfl por preTrained si se usa el modelo guardado\n",
    "test_accuracy = accuracy_score(y_test_np, y_pred)\n",
    "\n",
    "print(\"\\n--- Resultados de la Evaluación Final ---\")\n",
    "print(f\"Precisión (Accuracy) en el conjunto de prueba: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test_np, y_pred, target_names=['No Diabetes', 'Diabetes']))\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "cm = confusion_matrix(y_test_np, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])\n",
    "plt.xlabel('Prediction Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCurva de Pérdida del Entrenamiento:\")\n",
    "train_loss = clf.history['loss']\n",
    "val_loss = clf.history['val_0_logloss']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, label='Train Loss (Logloss)')\n",
    "plt.plot(val_loss, label='Validation Loss (Logloss)')\n",
    "plt.title('Curva de Loss del Entrenamiento')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss train')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86606706",
   "metadata": {},
   "source": [
    "# GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "# --- Parámetros de la GAN y del Entrenamiento ---\n",
    "LATENT_DIM = 100\n",
    "EPOCHS = 0 \n",
    "BATCH_SIZE = 64\n",
    "SAMPLE_INTERVAL = 1 \n",
    "LEARNING_RATE_G = 0.0002\n",
    "LEARNING_RATE_D = 0.00001\n",
    "BETA1 = 0.5 \n",
    "SAVE_INTERVAL = 100\n",
    "\n",
    "iteration = 3\n",
    "generator_path = \"/content/gdrive/My Drive/G11/Guardar_modelo/GANs/Multi/generator_gan_pytorch.pth\"\n",
    "discriminator_path = \"/content/gdrive/My Drive/G11/Guardar_modelo/GANs/Multi/discriminator_gan_pytorch.pth\"\n",
    "\n",
    "# Configurar dispositivo (GPU si está disponible)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# --- Cargar el archivo CSV original ---\n",
    "input_file = '/content/gdrive/My Drive/G11/Datasets/datasetMulti.csv'\n",
    "data = pd.read_csv(input_file)\n",
    "# --- Eliminar columnas de identificación ---\n",
    "data = data.drop(columns=['ID', 'No_Pation'])\n",
    "\n",
    "# --- Estandatiza valores en las columnas CLASS y Gender ya que hay alguno ---\n",
    "data['CLASS'] = data['CLASS'].astype(str).str.strip()\n",
    "data['Gender'] = data['Gender'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Mapear valores de 'Gender' a numéricos \n",
    "data['Gender'] = data['Gender'].map({'F': 0, 'M': 1})\n",
    "\n",
    "# Mapear 'CLASS' (ahora 'CLASS') a valores numéricos \n",
    "class_mapping = {'N': 0, 'P': 1, 'Y': 2}\n",
    "data['CLASS'] = data['CLASS'].map(class_mapping)\n",
    "\n",
    "original_columns = data.columns.tolist()\n",
    "\n",
    "# --- Verificar el conteo de clases antes de SMOTE ---\n",
    "print(\"\\n--- Conteo de valores para CLASS (Original antes de SMOTE): ---\")\n",
    "print(data['CLASS'].value_counts().sort_index())\n",
    "\n",
    "# --- 2. Aplicar SMOTE para balancear las clases ---\n",
    "print(\"\\nIniciando balanceo con SMOTE...\")\n",
    "\n",
    "# Separar características (X) y variable objetivo (y)\n",
    "X_smote = data.drop('CLASS', axis=1)\n",
    "y_smote = data['CLASS']\n",
    "\n",
    "# Definir la estrategia de muestreo para que todas tengan 844\n",
    "sampling_strategy = {0: 844, 1: 844, 2: 844} \n",
    "\n",
    "#Se aplica SMOTE\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_smote, y_smote)\n",
    "\n",
    "# Recombinar los datos balanceados en un DataFrame para el preprocesamiento de la GAN\n",
    "data_df = pd.DataFrame(X_resampled, columns=X_smote.columns)\n",
    "data_df['CLASS'] = y_resampled\n",
    "\n",
    "# --- Verificar el conteo de clases después de SMOTE ---\n",
    "print(\"\\n--- Conteo de valores para CLASS (Después de SMOTE): ---\")\n",
    "print(data_df['CLASS'].value_counts().sort_index())\n",
    "\n",
    "# --- 3. Preprocesamiento ---\n",
    "print(\"Iniciando preprocesamiento...\")\n",
    "processed_data_parts = []\n",
    "scalers_dict = {}\n",
    "column_info_for_generator_output = []\n",
    "\n",
    "special_cols_log_scale = ['AGE', 'BMI']\n",
    "diabetes_col = 'CLASS'\n",
    "gender_col = 'Gender' # Nueva columna categórica\n",
    "\n",
    "# B. Gender (One-Hot Encoding)\n",
    "num_classes_gender = data_df[gender_col].nunique() # Debería ser 2 (M/F)\n",
    "gender_one_hot = np.eye(num_classes_gender)[data_df[gender_col].astype(int)]\n",
    "processed_data_parts.append(gender_one_hot)\n",
    "column_info_for_generator_output.append({'name': gender_col, 'type': 'one_hot', 'num_classes': num_classes_gender})\n",
    "print(f\"Preprocesado {gender_col} con One-Hot Encoding. Shape: {gender_one_hot.shape}\")\n",
    "\n",
    "# C. Columnas con log1p + MinMaxScaler\n",
    "for col_name in special_cols_log_scale:\n",
    "    original_values = data_df[col_name].values.reshape(-1, 1)\n",
    "    log_transformed_values = np.log1p(original_values)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_values = scaler.fit_transform(log_transformed_values)\n",
    "    processed_data_parts.append(scaled_values)\n",
    "    scalers_dict[col_name] = {'scaler': scaler, 'log_applied': True, 'original_min': original_values.min(), 'original_max': original_values.max()}\n",
    "    column_info_for_generator_output.append({'name': col_name, 'type': 'scaled_continuous_sigmoid'})\n",
    "    print(f\"Preprocesado {col_name} con log1p + MinMaxScaler. Shape: {scaled_values.shape}\")\n",
    "\n",
    "# D. Otras columnas (MinMaxScaler para [0,1])\n",
    "\n",
    "other_cols = [col for col in original_columns if col not in [gender_col] + special_cols_log_scale]\n",
    "for col_name in other_cols:\n",
    "    original_values = data_df[col_name].values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_values = scaler.fit_transform(original_values)\n",
    "    processed_data_parts.append(scaled_values)\n",
    "    scalers_dict[col_name] = {'scaler': scaler, 'log_applied': False, 'original_min': original_values.min(), 'original_max': original_values.max()}\n",
    "    column_info_for_generator_output.append({'name': col_name, 'type': 'scaled_continuous_sigmoid'})\n",
    "    print(f\"Preprocesado {col_name} con MinMaxScaler. Shape: {scaled_values.shape}\")\n",
    "\n",
    "X_train_processed_np = np.concatenate(processed_data_parts, axis=1).astype(np.float32)\n",
    "DATA_DIM = X_train_processed_np.shape[1]\n",
    "print(f\"Forma final de los datos procesados (X_train_processed_np): {X_train_processed_np.shape}\")\n",
    "\n",
    "# Convertir datos a tensores de PyTorch\n",
    "X_train_tensor = torch.tensor(X_train_processed_np, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_resampled.astype(int), dtype=torch.long).to(device)\n",
    "num_classes = len(np.unique(y_resampled))\n",
    "\n",
    "# --- 3. Definir el modelo GAN (PyTorch) ---\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, n_classes, data_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + n_classes, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.BatchNorm1d(512, momentum=0.8),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(1024, momentum=0.8),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm1d(2048, momentum=0.8),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.BatchNorm1d(4096, momentum=0.8),\n",
    "            nn.Linear(4096, data_dim),\n",
    "            nn.Sigmoid() # Salida general en [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z, class_onehot):\n",
    "        x = torch.cat([z, class_onehot], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_dim, n_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim + n_classes, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid() # Salida binaria (real/falso)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, class_onehot):\n",
    "        x = torch.cat([x, class_onehot], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Inicializar generador y discriminador\n",
    "generator = Generator(LATENT_DIM, num_classes, DATA_DIM).to(device)\n",
    "discriminator = Discriminator(DATA_DIM, num_classes).to(device)\n",
    "\n",
    "# Load models\n",
    "generator.load_state_dict(torch.load(generator_path, map_location=device))\n",
    "discriminator.load_state_dict(torch.load(discriminator_path, map_location=device))\n",
    "\n",
    "# Función de pérdida\n",
    "adversarial_loss = nn.BCELoss().to(device)\n",
    "\n",
    "# Optimizadores\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE_G, betas=(BETA1, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_D, betas=(BETA1, 0.999))\n",
    "\n",
    "print(\"\\n--- Arquitectura del Generador (PyTorch) ---\")\n",
    "print(generator)\n",
    "print(\"\\n--- Arquitectura del Discriminador (PyTorch) ---\")\n",
    "print(discriminator)\n",
    "\n",
    "\n",
    "# --- 4. Bucle de Entrenamiento (PyTorch) ---\n",
    "print(\"\\nIniciando entrenamiento de la GAN con PyTorch...\")\n",
    "d_loss_history = []\n",
    "g_loss_history = []\n",
    "d_acc_history = [] # Para la precisión del discriminador\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    perm = torch.randperm(X_train_tensor.size(0))\n",
    "    X_train_shuffled = X_train_tensor[perm]\n",
    "    y_train_shuffled = y_train_tensor[perm]\n",
    "\n",
    "    d_loss_epoch = 0\n",
    "    g_loss_epoch = 0\n",
    "    num_batches = X_train_shuffled.size(0) // BATCH_SIZE\n",
    "    for i in range(num_batches): \n",
    "        # ---------------------\n",
    "        #  Entrenar Discriminador\n",
    "        # ---------------------\n",
    "        discriminator.train()\n",
    "        generator.eval() \n",
    "\n",
    "        batch_classes = y_train_tensor[perm][i*BATCH_SIZE:(i+1)*BATCH_SIZE] \n",
    "        batch_classes_onehot = torch.nn.functional.one_hot(batch_classes, num_classes).float() \n",
    "\n",
    "        # Datos reales\n",
    "        real_imgs = X_train_shuffled[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        real_classes = y_train_shuffled[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        real_classes_onehot = torch.nn.functional.one_hot(real_classes, num_classes).float()\n",
    "        real_imgs += 0.1 * torch.randn_like(real_imgs)\n",
    "        real_labels = torch.full((real_imgs.size(0), 1), 0.9, device=device)\n",
    "\n",
    "        # Datos falsos\n",
    "        noise = torch.randn(real_imgs.size(0), LATENT_DIM, dtype=torch.float32).to(device)\n",
    "        fake_classes = torch.randint(0, num_classes, (real_imgs.size(0),), device=device)\n",
    "        fake_classes_onehot = torch.nn.functional.one_hot(fake_classes, num_classes).float()\n",
    "        fake_imgs = generator(noise, fake_classes_onehot)\n",
    "        fake_imgs += 0.1 * torch.randn_like(fake_imgs)\n",
    "        fake_labels = torch.full((fake_imgs.size(0), 1), 0.1, device=device)\n",
    "\n",
    "        # Entrenar con datos reales\n",
    "        optimizer_D.zero_grad()\n",
    "        real_output = discriminator(real_imgs, real_classes_onehot)\n",
    "        d_loss_real = adversarial_loss(real_output, real_labels)\n",
    "\n",
    "        # Entrenar con datos falsos\n",
    "        fake_output = discriminator(fake_imgs.detach(), fake_classes_onehot)\n",
    "        d_loss_fake = adversarial_loss(fake_output, fake_labels)\n",
    "        d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "        d_loss_epoch += d_loss.item()\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Entrenar Generador\n",
    "        # ---------------------\n",
    "        generator.train() \n",
    "        for _ in range(4):\n",
    "          optimizer_G.zero_grad()\n",
    "          #Generacion de datos con generator\n",
    "          noise_g = torch.randn(BATCH_SIZE, LATENT_DIM, dtype=torch.float32).to(device) \n",
    "          gen_classes = torch.randint(0, num_classes, (BATCH_SIZE,), device=device)\n",
    "          gen_classes_onehot = torch.nn.functional.one_hot(gen_classes, num_classes).float()\n",
    "          gen_imgs_for_g = generator(noise_g, gen_classes_onehot)\n",
    "          real_labels_for_g = torch.ones(gen_imgs_for_g.size(0), 1, dtype=torch.float32).to(device)\n",
    "          #verifica dato creado en el discrimidar\n",
    "          output_g = discriminator(gen_imgs_for_g, gen_classes_onehot)\n",
    "          #Calcula la loss del generador\n",
    "          g_loss = adversarial_loss(output_g, real_labels_for_g)\n",
    "          g_loss_epoch += g_loss.item()\n",
    "          g_loss.backward()\n",
    "          optimizer_G.step()\n",
    "\n",
    "    # Guardar el progreso al final de la época (promedio si se quiere)\n",
    "    d_loss_history.append(d_loss_epoch / num_batches)\n",
    "    g_loss_history.append(g_loss_epoch / (num_batches*4))\n",
    "\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        #torch.save(generator.state_dict(), generator_path)\n",
    "        #torch.save(discriminator.state_dict(), discriminator_path)\n",
    "        print(f\"Modelos guardados en '{generator_path}' y '{discriminator_path}'\")\n",
    "\n",
    "    if (epoch + 1) % SAMPLE_INTERVAL == 0:\n",
    "        print(f\"{epoch + 1}/{EPOCHS} [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "\n",
    "# --- Guardar modelos entrenados ---\n",
    "#torch.save(generator.state_dict(), generator_path)\n",
    "#torch.save(discriminator.state_dict(), discriminator_path)\n",
    "#print(f\"Modelos guardados en '{generator_path}' y '{discriminator_path}'\")\n",
    "\n",
    "# --- Graficar historial de pérdidas ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(d_loss_history, label='Discriminator Loss')\n",
    "plt.plot(g_loss_history, label='Generator Loss')\n",
    "plt.title(\"Historial de Pérdidas de la GAN (PyTorch)\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.legend()\n",
    "plt.savefig(f'Guardar_modelo/GANs/Multi/gan_loss_history_pytorch_{EPOCHS}_{iteration}.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 5. Generación y Postprocesamiento de Datos Finales (PyTorch) ---\n",
    "print(\"\\nGenerando datos sintéticos finales con PyTorch...\")\n",
    "num_samples_per_class = 50000  # Numero de datos a generar pr clase\n",
    "num_classes = data_df['CLASS'].nunique()\n",
    "generation_batch_size = 512\n",
    "\n",
    "generator.eval()\n",
    "all_class_labels = []\n",
    "synthetic_df_final = pd.DataFrame()\n",
    "all_generated_data_scaled = []\n",
    "\n",
    "#Generacion de datos poor Batches\n",
    "with torch.no_grad():\n",
    "    for class_value in range(num_classes):\n",
    "        samples_generated = 0\n",
    "        while samples_generated < num_samples_per_class:\n",
    "            current_batch_size = min(generation_batch_size, num_samples_per_class - samples_generated)\n",
    "            noise = torch.randn(current_batch_size, LATENT_DIM, dtype=torch.float32).to(device)\n",
    "            class_onehot = torch.zeros(current_batch_size, num_classes, device=device)\n",
    "            class_onehot[:, class_value] = 1\n",
    "            generated_batch_scaled = generator(noise, class_onehot).cpu().numpy()\n",
    "            all_generated_data_scaled.append(generated_batch_scaled)\n",
    "            all_class_labels.extend([class_value] * current_batch_size)\n",
    "            samples_generated += current_batch_size\n",
    "\n",
    "\n",
    "generated_data_scaled_final_np = np.concatenate(all_generated_data_scaled, axis=0)\n",
    "all_class_labels = np.array(all_class_labels)\n",
    "\n",
    "synthetic_df_final['CLASS'] = all_class_labels\n",
    "\n",
    "#se reajustan los valores con el one_hot_encoding y se mapean lo datos devuelta a etiquetas Originales\n",
    "current_col_idx_in_generated = 0\n",
    "for col_info in column_info_for_generator_output:\n",
    "    col_name = col_info['name']\n",
    "    col_type = col_info['type']\n",
    "\n",
    "    if col_type == 'one_hot':\n",
    "        num_classes_other = col_info['num_classes']\n",
    "        one_hot_part = generated_data_scaled_final_np[:, current_col_idx_in_generated : current_col_idx_in_generated + num_classes_other]\n",
    "        decoded_classes = np.argmax(one_hot_part, axis=1)\n",
    "        synthetic_df_final[col_name] = decoded_classes\n",
    "        # Si 'Gender' o 'CLASS', mapear de nuevo a sus etiquetas originales si es necesario para la visualización/guardado\n",
    "        if col_name == 'CLASS':\n",
    "            # Invertir el mapeo {'N': 0, 'P': 1, 'Y': 2}\n",
    "            reverse_class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "            synthetic_df_final[col_name] = synthetic_df_final[col_name].map(reverse_class_mapping)\n",
    "        elif col_name == 'Gender':\n",
    "            # Invertir el mapeo {'F': 0, 'M': 1}\n",
    "            reverse_gender_mapping = {0: 'F', 1: 'M'}\n",
    "            synthetic_df_final[col_name] = synthetic_df_final[col_name].map(reverse_gender_mapping)\n",
    "        current_col_idx_in_generated += num_classes_other\n",
    "\n",
    "    elif col_type == 'scaled_continuous_sigmoid':\n",
    "        generated_values_scaled = generated_data_scaled_final_np[:, current_col_idx_in_generated : current_col_idx_in_generated + 1]\n",
    "        current_col_idx_in_generated += 1\n",
    "\n",
    "        s_info = scalers_dict[col_name]\n",
    "        scaler_obj = s_info['scaler']\n",
    "        inverted_values = scaler_obj.inverse_transform(generated_values_scaled)\n",
    "\n",
    "        if s_info['log_applied']:\n",
    "            inverted_values = np.expm1(inverted_values)\n",
    "        if data_df[col_name].dtype == 'int64' or (data_df[col_name].dtype == 'float64' and np.all(data_df[col_name] == data_df[col_name].astype(int))):\n",
    "            final_values = np.round(inverted_values)\n",
    "        else:\n",
    "            final_values = inverted_values\n",
    "\n",
    "        final_values = np.clip(final_values, s_info['original_min'], s_info['original_max'])\n",
    "        synthetic_df_final[col_name] = final_values.flatten().astype(data_df[col_name].dtype)\n",
    "\n",
    "synthetic_df_final = synthetic_df_final[original_columns]\n",
    "\n",
    "\n",
    "temp_df_for_balancing = synthetic_df_final.copy()\n",
    "print(\"Value counts in temp_df_for_balancing['CLASS'] before balancing:\")\n",
    "print(temp_df_for_balancing['CLASS'].value_counts())\n",
    "\n",
    "#se balancea lo generado en caso de no salir balanceado\n",
    "balanced_synthetic = []\n",
    "for class_value in range(num_classes):\n",
    "    class_samples = temp_df_for_balancing[temp_df_for_balancing['CLASS'] == class_value]\n",
    "    if len(class_samples) == 0:\n",
    "        print(f\"Warning: No synthetic samples found for class {class_value}. Skipping this class.\")\n",
    "        continue\n",
    "    balanced_synthetic.append(class_samples.sample(n=num_samples_per_class, replace=True, random_state=42))\n",
    "synthetic_df_final_balanced = pd.concat(balanced_synthetic).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "reverse_class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "synthetic_df_final_balanced['CLASS'] = synthetic_df_final_balanced['CLASS'].map(reverse_class_mapping)\n",
    "\n",
    "#Guarda los datos generados en un CSV\n",
    "output_file_pytorch = f'/content/gdrive/My Drive/G11/Datasets/generated_data_gan_pytorch_{EPOCHS}_{iteration}.csv'\n",
    "synthetic_df_final_balanced.to_csv(output_file_pytorch, index=False)\n",
    "print(f\"\\nDatos sintéticos generados y guardados en: {output_file_pytorch}\")\n",
    "\n",
    "# --- Mostrar algunas estadísticas de los datos generados (igual que antes) ---\n",
    "print(\"\\n--- Descripción de los datos originales: ---\")\n",
    "print(data_df.describe())\n",
    "print(\"\\n--- Descripción de los datos sintéticos (PyTorch): ---\")\n",
    "print(synthetic_df_final_balanced.describe())\n",
    "\n",
    "print(\"\\n--- Conteo de valores para CLASS (Original): ---\")\n",
    "print(data_df['CLASS'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\n--- Conteo de valores para CLASS (Sintético - PyTorch): ---\")\n",
    "print(synthetic_df_final_balanced['CLASS'].value_counts(normalize=True).sort_index())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.histplot(data_df['AGE'], ax=axes[0], color='blue', label='Original', kde=True, stat=\"density\")\n",
    "sns.histplot(synthetic_df_final_balanced['AGE'], ax=axes[0], color='green', label='Sintético (PyTorch)', kde=True, stat=\"density\")\n",
    "axes[0].set_title('Distribución de AGE')\n",
    "axes[0].legend()\n",
    "\n",
    "sns.histplot(data_df['BMI'], ax=axes[1], color='blue', label='Original', kde=True, stat=\"density\")\n",
    "sns.histplot(synthetic_df_final_balanced['BMI'], ax=axes[1], color='green', label='Sintético (PyTorch)', kde=True, stat=\"density\")\n",
    "axes[1].set_title('Distribución de BMI')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'Guardar_modelo/GANs/Multi/generated_data_distributions_comparison_pytorch_{EPOCHS}_{iteration}.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinalizado con PyTorch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8297531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Función para comparar y visualizar distribuciones ---\n",
    "# esto es solo para verificacion manual para saber como estan resultando los datos generados\n",
    "def compare_data_distributions(real_df, generated_df, numerical_cols, categorical_cols, output_prefix=\"comparison\"):\n",
    "    print(\"\\n--- Iniciando comparación de distribuciones (Reales vs. Sintéticas) ---\")\n",
    "\n",
    "    # 1. Comparación de Estadísticas Descriptivas Generales\n",
    "    print(\"\\n--- Estadísticas Descriptivas - Datos Originales (después de SMOTE) ---\")\n",
    "    print(real_df.describe())\n",
    "    print(\"\\n--- Estadísticas Descriptivas - Datos Sintéticos ---\")\n",
    "    print(generated_df.describe())\n",
    "\n",
    "    # 2. Comparación de Conteo de Valores (para Categóricas) y Varianzas (para Numéricas)\n",
    "    print(\"\\n--- Conteo de Clases / Valores (Categóricas) ---\")\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nColumna: {col}\")\n",
    "        print(\"Real:\")\n",
    "        print(real_df[col].value_counts(normalize=True).sort_index())\n",
    "        print(\"Sintético:\")\n",
    "        print(generated_df[col].value_counts(normalize=True).sort_index())\n",
    "\n",
    "    print(\"\\n--- Varianzas de Columnas Numéricas ---\")\n",
    "    real_variances = real_df[numerical_cols].var()\n",
    "    gen_variances = generated_df[numerical_cols].var()\n",
    "    comparison_variances = pd.DataFrame({'Real_Variance': real_variances, 'Synthetic_Variance': gen_variances})\n",
    "    print(comparison_variances)\n",
    "\n",
    "    # 3. Visualización de Histogramas/KDE (para Numéricas)\n",
    "    print(\"\\n--- Visualizando distribuciones numéricas ---\")\n",
    "    num_plots_per_row = 3\n",
    "    num_rows_numerical = (len(numerical_cols) + num_plots_per_row - 1) // num_plots_per_row\n",
    "    fig_num, axes_num = plt.subplots(num_rows_numerical, num_plots_per_row, figsize=(5 * num_plots_per_row, 4 * num_rows_numerical))\n",
    "    axes_num = axes_num.flatten() \n",
    "\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        sns.histplot(real_df[col], ax=axes_num[i], color='blue', label='Real', kde=True, stat=\"density\", alpha=0.6)\n",
    "        sns.histplot(generated_df[col], ax=axes_num[i], color='green', label='Sintético', kde=True, stat=\"density\", alpha=0.6)\n",
    "        axes_num[i].set_title(f'Distribución de {col}')\n",
    "        axes_num[i].legend()\n",
    "\n",
    "    # Ocultar ejes vacíos si hay menos subplots que el espacio total\n",
    "    for j in range(i + 1, len(axes_num)):\n",
    "        fig_num.delaxes(axes_num[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_prefix}_numerical_distributions.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Visualización de Gráficos de Barras (para Categóricas)\n",
    "    print(\"\\n--- Visualizando distribuciones categóricas ---\")\n",
    "    num_plots_per_row = 2\n",
    "    num_rows_categorical = (len(categorical_cols) + num_plots_per_row - 1) // num_plots_per_row\n",
    "    fig_cat, axes_cat = plt.subplots(num_rows_categorical, num_plots_per_row, figsize=(6 * num_plots_per_row, 5 * num_rows_categorical))\n",
    "    axes_cat = axes_cat.flatten()\n",
    "\n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        real_counts = real_df[col].value_counts(normalize=True).sort_index()\n",
    "        gen_counts = generated_df[col].value_counts(normalize=True).sort_index()\n",
    "        all_categories = real_counts.index.union(gen_counts.index)\n",
    "\n",
    "        df_plot = pd.DataFrame({\n",
    "            'Category': all_categories,\n",
    "            'Real': real_counts.reindex(all_categories, fill_value=0),\n",
    "            'Sintético': gen_counts.reindex(all_categories, fill_value=0)\n",
    "        }).melt(id_vars='Category', var_name='Dataset', value_name='Proportion')\n",
    "\n",
    "        sns.barplot(x='Category', y='Proportion', hue='Dataset', data=df_plot, ax=axes_cat[i], palette={'Real': 'blue', 'Sintético': 'green'})\n",
    "        axes_cat[i].set_title(f'Distribución de {col}')\n",
    "        axes_cat[i].set_ylabel('Proporción')\n",
    "\n",
    "    for j in range(i + 1, len(axes_cat)):\n",
    "        fig_cat.delaxes(axes_cat[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_prefix}_categorical_distributions.png\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Fin de la comparación de distribuciones ---\")\n",
    "\n",
    "# Define tus columnas numéricas y categóricas\n",
    "numerical_features_for_comparison = ['AGE', 'Urea', 'Cr', 'HbA1c', 'Chol', 'TG', 'HDL', 'LDL', 'VLDL', 'BMI']\n",
    "categorical_features_for_comparison = ['Gender', 'CLASS']\n",
    "\n",
    "# Llama a la función de comparación\n",
    "compare_data_distributions(\n",
    "    real_df=data_df, \n",
    "    generated_df=synthetic_df_final_balanced, \n",
    "    numerical_cols=numerical_features_for_comparison,\n",
    "    categorical_cols=categorical_features_for_comparison,\n",
    "    output_prefix=\"gan_data_comparison\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0afb1f",
   "metadata": {},
   "source": [
    "# MLP Multiclase mixto entre dataset real y generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b55989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejo de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Modelos de Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Preprocesamiento de datos\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Gráficos\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configuración warnings (Quitar en caso de errores desconocidos)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Versión de paquetes usados\n",
    "color = '\\033[1m\\033[38;5;208m'\n",
    "print(f\"{color}Versión de las librerias utilizadas:\")\n",
    "print(f\"{color}- Version torch: {torch.__version__}\")\n",
    "print(f\"{color}- Version pandas: {pd.__version__}\")\n",
    "print(f\"{color}- Version numpy: {np.__version__}\")\n",
    "print(f\"{color}- Version sklearn: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce83e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero de datos por clase que tendra el dataset mezclado\n",
    "num_sample_per_class = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea41333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar Dataset Original\n",
    "try:\n",
    "    data = pd.read_csv(f'/content/gdrive/My Drive/G11/Datasets/datasetMulti_original.csv') \n",
    "    print(\"Archivo cargado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67215522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar Dataset generado por el modelo GANs\n",
    "try:\n",
    "    data_gen = pd.read_csv(f'/content/gdrive/My Drive/G11/Datasets/datasetMulti_Gen.csv') \n",
    "    print(\"Archivo cargado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas de identificación\n",
    "columns_to_drop = ['ID', 'No_Pation']\n",
    "existing_columns = [col for col in columns_to_drop if col in data.columns]\n",
    "if existing_columns:\n",
    "    data = data.drop(columns=['ID', 'No_Pation'])\n",
    "\n",
    "# Estandatiza valores en las columnas CLASS y Gender ya que hay alguno \n",
    "data['CLASS'] = data['CLASS'].astype(str).str.strip()\n",
    "data['Gender'] = data['Gender'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Mapear la columna objetivo 'CLASS' a valores numéricos\n",
    "class_mapping = {'N': 0, 'P': 1, 'Y': 2}\n",
    "data['CLASS'] = data['CLASS'].map(class_mapping)\n",
    "\n",
    "# Convertir la variable categórica 'Gender' a numérica\n",
    "data = pd.get_dummies(data, columns=['Gender'], drop_first=True)\n",
    "\n",
    "# Eliminar filas con valores nulos si los hubiera\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Renombrar columnas para que no tengan caracteres especiales\n",
    "data.columns = data.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "\n",
    "data_sample = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapear la columna objetivo 'CLASS' a valores numéricos\n",
    "class_mapping = {'N': 0, 'P': 1, 'Y': 2}\n",
    "data_gen['CLASS'] = data_gen['CLASS'].map(class_mapping)\n",
    "\n",
    "# Convertir la variable categórica 'Gender' a numérica\n",
    "data_gen = pd.get_dummies(data_gen, columns=['Gender'], drop_first=True)\n",
    "\n",
    "# Eliminar filas con valores nulos si los hubiera\n",
    "data_gen.dropna(inplace=True)\n",
    "\n",
    "# Renombrar columnas para que no tengan caracteres especiales\n",
    "data_gen.columns = data_gen.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "\n",
    "data_gen_sample = data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2291c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 110425\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resample de dataset manteniendo el original y agregando datos del dataset generado a las clases que tengan menos datos que los establecidos en num_sample_per_class\n",
    "data_count = data['CLASS'].value_counts()\n",
    "if ((0 in data_count) and (data_count[0]<num_sample_per_class)):\n",
    "    class_0 = data_gen[data_gen['CLASS'] == 0].sample(n=num_sample_per_class-data_count[0], random_state=42)\n",
    "if ((1 in data_count) and (data_count[1]<num_sample_per_class)):\n",
    "    class_1 = data_gen[data_gen['CLASS'] == 1].sample(n=num_sample_per_class-data_count[1], random_state=42)\n",
    "if ((2 in data_count) and (data_count[2]<num_sample_per_class)):\n",
    "    class_2 = data_gen[data_gen['CLASS'] == 2].sample(n=num_sample_per_class-data_count[2], random_state=42) \n",
    "balanced_train_data_gen = pd.concat([class_0, class_1,class_2]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "data_gen_sample = balanced_train_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Separar características (x) y objetivo (y) de la muestra\n",
    "x = data_sample.drop(columns=[\"CLASS\"]).to_numpy()\n",
    "y = data_sample[\"CLASS\"].to_numpy()\n",
    "\n",
    "# 2. Dividir en conjuntos de entrenamiento, validación \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42, stratify=y) # 20% para test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42, stratify=y_train) # 15% del resto para validación\n",
    "\n",
    "x_gen = data_gen_sample.drop(columns=[\"CLASS\"]).to_numpy()\n",
    "y_gen = data_gen_sample[\"CLASS\"].to_numpy()\n",
    "\n",
    "# 3. Dividir en conjuntos de entrenamiento, validación y prueba\n",
    "X_gen_train, X_gen_test, y_gen_train, y_gen_test = train_test_split(x_gen, y_gen, test_size=0.20, random_state=42, stratify=y_gen) # 20% para test\n",
    "X_gen_train, X_gen_val, y_gen_train, y_gen_val = train_test_split(X_gen_train, y_gen_train, test_size=0.15, random_state=42, stratify=y_gen_train) # 15% del resto para validación\n",
    "\n",
    "# 4. Regresar los datos a formato panda\n",
    "X_train_orig_pd = pd.DataFrame(X_train)\n",
    "y_train_orig_pd = pd.Series(y_train)\n",
    "X_test_orig_pd = pd.DataFrame(X_test)\n",
    "y_test_orig_pd = pd.Series(y_test)\n",
    "X_val_orig_pd = pd.DataFrame(X_val)\n",
    "y_val_orig_pd = pd.Series(y_val)\n",
    "\n",
    "X_gen_train_pd = pd.DataFrame(X_gen_train)\n",
    "y_gen_train_pd = pd.Series(y_gen_train)\n",
    "X_gen_test_pd = pd.DataFrame(X_gen_test)\n",
    "y_gen_test_pd = pd.Series(y_gen_test)\n",
    "X_gen_val_pd = pd.DataFrame(X_gen_val)\n",
    "y_gen_val_pd = pd.Series(y_gen_val)\n",
    "\n",
    "\n",
    "# 5. Concatena los entrenamiento, testeos y validaciones del dataset original con los del generado y los regresa a numpy\n",
    "X_train = (pd.concat([X_gen_train_pd, X_train_orig_pd]).sample(frac=1, random_state=42).reset_index(drop=True)).to_numpy()\n",
    "y_train = (pd.concat([y_gen_train_pd, y_train_orig_pd]).sample(frac=1, random_state=42).reset_index(drop=True)).to_numpy()\n",
    "\n",
    "X_test = (pd.concat([X_gen_test_pd, X_test_orig_pd]).sample(frac=1, random_state=42).reset_index(drop=True)).to_numpy()\n",
    "y_test = (pd.concat([y_gen_test_pd, y_test_orig_pd]).sample(frac=1, random_state=42).reset_index(drop=True)).to_numpy()\n",
    "\n",
    "X_val = (pd.concat([X_gen_val_pd, X_val_orig_pd]).sample(frac=1, random_state=42).reset_index(drop=True)).to_numpy()\n",
    "y_val = (pd.concat([y_gen_val_pd, y_val_orig_pd]).sample(frac=1, random_state=42).reset_index(drop=True)).to_numpy()\n",
    "\n",
    "# 6. Aplicar el escalado\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val) # Usar el mismo scaler del train\n",
    "X_test = scaler.transform(X_test) # Usar el mismo scaler del train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicar Tensores\n",
    "#Train\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "#Val\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "#Test\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2defdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir los Batchs del dataset\n",
    "batchsize = 32\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_data = DiabetesDataset(X_train, y_train)\n",
    "val_data = DiabetesDataset(X_val, y_val)\n",
    "test_data = DiabetesDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir el Modelo a usar\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hc1 = nn.Linear(x.shape[1], 256) # Ajuste automático al número de columnas\n",
    "        self.hc2 = nn.Linear(256, 128)\n",
    "        self.hc3 = nn.Linear(128, 64)\n",
    "        self.hc4 = nn.Linear(64, 3)\n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.dp1 = nn.Dropout(0.4)\n",
    "        self.dp2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        l1 = self.dp1(self.act(self.hc1(input)))\n",
    "        l2 = self.dp2(self.act(self.hc2(l1)))\n",
    "        l3 = self.act(self.hc3(l2))\n",
    "        output = self.hc4(l3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5086006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa el Modelo, define el Learning Rate, el optimizador y el loss\n",
    "testeo = MLP()\n",
    "lr = 1e-4\n",
    "opt = torch.optim.Adam(testeo.parameters(), lr=lr, weight_decay=1e-4)\n",
    "loss_func = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "testeo_path = f'/content/gdrive/My Drive/G11/Guardar_modelo/MLP/Multi/Multi_{num_sample_per_class}_pytorch.pth'\n",
    "testeo_path_FULL = f'/content/gdrive/My Drive/G11/Guardar_modelo/MLP/Multi/Multi_{num_sample_per_class}_pytorch_FULL.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar Pesos\n",
    "#testeo.load_state_dict(torch.load(testeo_path))\n",
    "#Cargar Modelo\n",
    "#testeo = torch.load(testeo_path_FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamiento\n",
    "\n",
    "n_epochs = 2000\n",
    "history = {\n",
    "    \"TL\" : [],\n",
    "    \"VL\" : []\n",
    "}\n",
    "early_stopping = {\n",
    "    \"delta\" : 1e-5,\n",
    "    \"patience\": 10\n",
    "}\n",
    "best_val_loss = float(\"inf\")\n",
    "aux = 0\n",
    "\n",
    "for i in range(n_epochs+1):\n",
    "    testeo.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:  \n",
    "      output = testeo(X_batch)\n",
    "      loss = loss_func(output, y_batch)  \n",
    "      \n",
    "      opt.zero_grad()\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    history[\"TL\"].append(epoch_loss)\n",
    "\n",
    "    testeo.eval()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in val_loader:\n",
    "      with torch.no_grad():\n",
    "        preds_val = testeo(X_batch)\n",
    "      val_loss = loss_func(preds_val, y_batch)\n",
    "\n",
    "      epoch_loss += val_loss.item()\n",
    "    epoch_loss /= len(val_loader)\n",
    "    history[\"VL\"].append(epoch_loss)\n",
    "    \n",
    "    if epoch_loss + early_stopping[\"delta\"] < best_val_loss:\n",
    "      best_val_loss = epoch_loss\n",
    "      aux = 0\n",
    "    else:\n",
    "      aux += 1\n",
    "      if aux >= early_stopping[\"patience\"]:\n",
    "        print(f\"Terminando el entrenamiento en la época {i}\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Pesos\n",
    "torch.save(testeo.state_dict(), testeo_path)\n",
    "#Guardar Modelo Completo\n",
    "torch.save(testeo, testeo_path_FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafico de loss en tre training y validacion\n",
    "plt.plot(history[\"TL\"], label=\"Train\")\n",
    "plt.plot(history[\"VL\"], label=\"Val\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11488ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo de accuracy del cnjunto de entrenamiento y de testeo\n",
    "#Train\n",
    "with torch.no_grad():\n",
    "    output_train = testeo(X_train)\n",
    "y_hat_train = output_train.numpy()\n",
    "y_hat_train = [np.argmax(i) for i in y_hat_train]\n",
    "print(f'Training Accuracy: {accuracy_score(y_train, y_hat_train):.2f}')\n",
    "#Testeo\n",
    "with torch.no_grad():\n",
    "    output = testeo(X_test)  \n",
    "y_hat = output.numpy()\n",
    "y_hat = [np.argmax(i) for i in y_hat]\n",
    "print(f'Testing Accuracy: {accuracy_score(y_test, y_hat):.2f}')\n",
    "\n",
    "#precision, recall f1-score support usando resultados de conjunto de testeo\n",
    "print(classification_report(y_test, y_hat, target_names=['N', 'P', 'Y']))\n",
    "\n",
    "cm =confusion_matrix(y_test, y_hat)\n",
    "cm_plot = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "cm_plot.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
